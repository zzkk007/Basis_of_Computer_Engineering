"-----------------------------------------------------------"

              《计算机组成结构化方法》 学习笔记

                         第一章 概述  

    数字计算机是通过执行人们给指令来完成工作的机器。描述如何完成一个确定的任务的指令序列称为程序。
    每台计算机的电路都只能识别和直接执行有限的简单指令，所有程序都必须在执行前转换成这些指令。
    
    计算机的这些原始指令共同组成了一种可供人和计算机进行交流的语言，我们成为其机器语言。
    原始指令应尽量简单，兼顾考虑计算机的使用要求和性能要求，以降低实现电路的成本和复杂度。
    
    通过对计算机的简单描述，我们可将计算机化为一系列抽象机，每台抽象机都建立在其下层抽象机的基础上。
    这样，计算机的复杂性就在可控制范围内，计算机系统的设计也可在有组织和系统的状态下进行。
    我们把这种方法称为结构化计算机组成。
    
    """1.1 机构化计算机组成"""
    
        正如前面提到的，在方便人们使用和方便计算机实现之间存在着巨大的差距。人们能要做 X,
        而计算机只会做 Y。 这就是问题，本书目的就是解释如何解决这个问题。
        
        1.1.1 语言、层次和虚拟机：
        
            这个问题可从两个途径解决，两者都需要设计一个比内置的机器指令更方便人们使用的新的指令集合。
            这样，新的机器指令集合也构成了一种语言，我们成为 L1,对应的把机器中内置的机器语言指令组成
            的语言叫L0, 两种途径的不同之处在于采取什么办法将只能执行用 L0写出的程序的计算机执行用
            L1写的程序。
                
                一种途径是在执行用 L1 写的程序之前生成一个等价的 L0 指令序列来替换它，生成的程序
                全部由 L0 指令组成。计算机执行等效的 L0 程序来替换原来的 L1 程序，这种技术叫作翻译（translation）
                
                另一种途径是用 L0 写一个程序，将 L1 的程序作为输入数据，按顺序检查它的每条指令，
                然后直接执行等效的 L0 指令序列计算出结果。它不需要事先生成一个 L0 语言的新程序。
                我们把这种方法称为解释（interpretation）,把完成这个过程的 L0 程序称为解释器。
                
            翻译和解释其实是类似的，两种方法中的 L1 的指令最终都通过执行等效的 L0 指令序列来实现。
            区别在于，翻译时整个 L1 程序都先转换为 L0 程序，然后 L1 程序被抛弃。新的 L0 程序被
            装入计算机内存中执行。执行过程中，运行的都是新生产的 L0 程序，控制计算机的也是 L0 程序。
                
            而解释时，每条 L1 指令被检查和解码之后将立即执行，不生成翻译后的程序。这里，控制计算机
            的是解释器。对他来说，L1 程序仅仅只是数据。
                
            比起理解翻译和解释着两种概念，想象存在一种假想的以 L1 为机器语言的计算机或虚拟机。
            让我们把这种虚拟机定义为 M1(相应的，把原来的以 L0 为机器语言的虚拟机定义为 M0),
            如果这种计算机可以足够低廉，那么就不需要 L0这种语言或者是执行L0语言的程序的机器了。
            人们可以简单的用 L1 写程序并让计算机直接执行。即使因为使用 L1 为语言的虚拟机太贵
            或太复杂而不能有电路构成，也可以写 L1 语言的程序，这些程序可以直接用被现有的计算机
            执行的 L0 语言程序翻译或解释。
            换句话说，完全可以像虚拟机真正的存在一样用它们的语言写程序。
                
            为了使翻译或解释实现可行，两种语言 L0 和 L1 的差别不能 “太” 大。
            这条限制意味着，虽然 L1 比 L0 好，但对于多数应用来说还是不理想。
            这也会导致对提出 L1 的最初目的-- 减轻程序员不得不用一种更适合计算机的语言来描述算法的负担
            有些失望。但不应该是绝望。
                
            显然，解决问题的办法是发明一种比 L1 更面向人且少面向机器的指令集来取代它，这种指令集
            形成的语言，我们可以称为 L2(对于的虚拟机为 M2)。人们可以像用 L2 作为机器语言的虚拟机
            真正存在一样用 L2 写程序，然后翻译成 L1 或用 L1 携程的解释器来执行。
                
            这种发明一系列的语言，每一种都比前一种更方便人们使用可以无限的继续下去，直到最后找到
            一种合适的语言。
            每一种语言都以前一种为基础，我们可以把使用这种技术的计算机看出一系列的层，一层在一层之上。
            最底部的语言或层最简单，而最上面的语言或层最复杂。
                
            语言和虚拟机之间存在着重要的对应关系，每种机器都有由它能执行的指令组成的机器语言，
            也就是说，机器定义语言。类似地，语言也定义了机器--即机器要能执行用这种语言写的所有程序。
            
            一般意义上讲，有 n 层的计算机可看成 n 台不同的虚拟机，每一台的机器语言都不相同。
            我们将交替使用术语 “层” 和 “虚拟机” 表示同样的意思。只有用 L0 语言写的程序可以被
            电子电路之直接执行，无须进行中间翻译或解释。用L1、L2、... Ln 写的程序必须经底层
            解释器解释或翻译成对于与底层的另一种语言。
        
        
        1.1.2 现代多层次计算机。
            
            
            第 5 层：面向问题的语言层-->(翻译，编译器)：
            
                第5层通常是提供解决现实问题的应用程序员使用的，这些语言通常称为高级语言。
                比如：c、c++、java、python、perl、php.
                用这些语言写的程序一般先由编译器（compiler）翻译成第 3或第4层语言，
                虽然偶尔也有解释执行的。例如，用java 语言写的程序通常先被翻译成一种
                类似于指令系统层的语言 --java 字节码，然后被解释执行。
            
            
            第 4 层：汇编语言层 --->(翻的译，汇编器)：
                
                第 4 层和第 3 层有着根本的区别，最低的三层并不是为普通程序员设计的，而是主要是为了支持
                高层所需的解释器或翻译器的运行而设计的，这些解释器和翻译器是由专职设计和实现新的虚拟机
                的系统程序员写的。第4层以及上各层才是提供哪些解决应用问题的应用程序使用的。
                
                第4层发生的其他变化是支持上层的方法。第2和3层都是解释，而第4、5层通常是翻译（不全是）。
                
                最低三层与第 4,5以及更高层的其他区别是提供语言本质的变化。
                第1、2、3层提供的机器语言都是数字串、这几层中的程序包含数字的长序、适合机器执行，而
                不容易被人理解。从第4层开始，提供的语言成了能帮助人们理解的单词或助记符。
                
                第4层，汇编语言层，实际上是某种底层语言的符号表示。本层为程序员写第1、2、3层程序提供了
                一种比用虚拟机语言直接写这些程序更舒服的方法。
                用汇编语言写的程序首先被翻译成第3、2、1层的语言，然后又相应的虚拟机或硬件解释执行。
                完成翻译过程的程序称为汇编器（assembler）。
            
            第 3 层：操作系统层 --->(部分解析，操作系统)：
            
                这一层是通常是混合层，大多数指令和 ISA 层相同，另外，这一层有新的指令集，
                不同的存储器结构，有同时运行两个或多个程序的能力，以及其他的一些特性。
                
                第3层增加的新的功能是由运行在第2层的解释器来执行的，那些和第2层相同的指令将直接交给微程序(或硬件)执行。
                有些指令由操作系统解释，而有些由微程序(或硬件)直接解释，这就是“混合层”的含义。
                这层又称为操作系统机器层。
                
            第 2 层：指令系统层 --->(解释（微程序）或直接执行)：
                
                指令系统层或 ISA 层（Instruction Set Architecture level），
                这一层规定了计算机使用的机器的指令集，实际上是由微程序解释或
                硬件执行电路直接执行的指令。
            
            
            第 1 层：微体系结构层 --->(硬件)：
                
                这层我们看到一般由 8 ~ 32个寄存器组成的寄存器组以及名为 ALU(算术逻辑部件)的电路，
                ALU 可以完成以下简单的算术运算。
                这些寄存器和ALU 相连形成数据通路（data path）,供数据在其流动。
                数据通路的基本功能是选择一个或两个寄存器作为 ALU 的操作数（例如，相加），然后将
                结果存回某个寄存器。
                
                一些机器上的数据通路的这些功能是由一个叫做微程序的程序控制的而另外一些机器是直接有硬件控制的。
                微程序可看作是对第 2 层指令的解释器，它通过数据通路逐条对执令进行取值、检查和执行。
                
            第 0 层：数据逻辑层：
                
                最低层--数据逻辑层，我们研究的对象是门（gate）,虽然它们由类似的原件（如晶体管）构成，
                但门可以作为数字设备的精确原型。每个门可以有一个或多个数字输入端（由 0 或 1 表示的信号）。
                可计算出输出这些输入的一些简单逻辑函数（与和或）的结果。门最多有几个晶体管构成，几个
                门可组成 1 位存储器，存放一个 0 或 1.
                1 位存储器可组合成 16、32、64一组，形成寄存器。每个寄存器可存放一个不大于某个最大值的二进制数。
                
            
            总体来说，将计算机设计成一些列的层，每层建立在它的前一层之上，每层表示一个不同的抽象，由不同的对象
            和操作表示。
           
            每层的数据类型、操作和特性构成了该层的体系结构。它解决的是该的层用户能看到的问题。
            计算机体系结构是研究如何设计程序员眼中的计算机系统的学科。
                                
        1.1.3 多层次计算机的演化：
            
            在计算机早期：
                硬件是具体的对象：集成电路、印制电路板、电缆、电源、存储器和打印机等。
                软件是组成程序的指令的集合，而不是记录它们的物理介质。
            
            随着时间推移，计算机层次增加、减少、合并，软硬件界限变得越来越模糊。
                硬件和软件在逻辑上是等同的。
                硬件就是固化的软件。
                任何硬件执行的指令都可由软件来模拟。
    
    1.2 计算机体系结构的里程碑：
    
        第一代电子管计算机 （ENIAC）
        第二代晶体管计算机 （IBM 7094）
        第三代集成电路计算机 （IBM 360）
        第四代个人计算机 （Intel 的 CPU 系列）
        第五代计算机智能手机            
        未来计算机将无处不在，嵌入到任何对象中。
    
    1.3 计算机家族：
    
        计算机行业的发展速度是其他行业无法比拟的，其最主要的原动力是每年增长的在单片芯片中集成更多集体管的能力。
        每片芯片集成的晶体管越多，即芯片中的逻辑门越多，也就意味着更大的内存和更强的处理能力。
         
        Moore定律：每片芯片中集成的晶体管的数量18个月翻一番，也就是说，每年增长 60%。
        Moore 定律不是一条定律，只是对固体物理学家和工艺工程师推动芯片发展工作的观察而得出的经验公式，并预测未来还会保持这个发展速度。
        
        推动技术进步的另一个因素是 Nathan 的软件第一个定律：软件是一种可以膨胀到充满整个容器的气体。
        20世纪80年代字处理软件占用几十K内存，而现在占用几十M内存。软件不断发展的特点对更快的处理器
        跟大的内存、更强的输入/输出 能力提供了持续要求。
        
        一次性计算机： 无线射频识别（Radio Frequency IDentification, RFID）芯片。
        
        微型控制器：嵌入式计算机，又称微型控制器，管理设备并与用户交互。
        
        移动计算机和游戏计算机：对移动计算机一个要求是移动中尽量减少能量消耗。
        
        个人计算机：
        
        服务器：从体系结构上看，单处理器的服务器和单处理器的个人计算机并没有真正的差别，
                仅仅是更快一些，体积大些，硬盘空间大一些，网络快一些。
                
        集群：由于服务器性价比上的持续增长，近年来，系统设计师们开始讲它们连接起来，共同组成集群。
             集群由标准的服务器级别系统通过 Gbps 的网络连接而成，运行特殊的软件使所有的共同解决一个商务、科学、工程问题。 
             通常，大集群被放置在一个专门的房间或者建筑中，称为数据中心。
             
             特定时代下流行的计算模式与当时的技术、经济和可以获得的应用密切相关，当这些要素改变时，计算模式随之改变。
         
             
    1.4 系列计算机举例：
    
        a. x86 体系结构简介(intel)：
            
            x86 架构可以在几乎所有的个人计算机和服务器系统中。
            
            X86架构：
                X86架构（The X86 architecture）是微处理器执行的计算机语言指令集，
                指一个intel通用计算机系列的标准编号缩写，也标识一套通用的计算机指令集合。
                
                X86是由Intel推出的一种复杂指令集，用于控制芯片的运行的程序，现在X86已经广泛运用到了家用PC（
                
            CPU的主频:
                
                即CPU内核工作的时钟频率（CPU Clock Speed）。
                通常所说的某某CPU是多少兆赫的，而这个多少兆赫就是“CPU的主频”。
                主频越高，CPU的运算速度就越快。但主频不等于处理器一秒钟执行的指令条数，因为一条指令的执行可能需要多个时钟周期。
                对于CPU，在有兼容性的前提下，主要看其速度，而主频越高，字节越长，CPU速度就越快。             
            
            
            高速缓存：
                
                用来在 CPU 内部或靠近 CPU 的地方存放最常用的内存字，以避免（或减少）内存访问。

            虽然 Moore 定律与存储器的位数有关，但它同样适用于 CPU 芯片，但是一个问题开始为它
            投上了阴影，即散热问题，时钟频率越高，要求电源电源越高，晶体管的数据少时还可以承受。
            而能耗及散热量与电压的平方成正比，因此，速度越快意味着排出的热量越多。主频越高，问题越严重。
            
            在计算机主频上的无情竞争似乎已经结束，取而代之的是，Intel 设计将两个或更多个 CPU 放到一个芯片上，
            同时还包含大容量的共享高速缓存。
            
            Moore 定律将来保持有效的途径可能是芯片中设计更多的核和更大片的高速缓存，而是不越来越快的时钟速度。
            如何充分发挥多核优势给程序员提出了更大的挑战，现有的编程方法针对复杂的单核微体系结构而提高程序性
            能是有效的，但多核要求程序从全新的角度考虑编程，通过精心设计程序的并行执行、利用线程、信号量、共享内存
            以及令人头疼以及引入错误的技术来提高程序的性能。
        
        b. ARM 体系结构简介(ARM 公司 Advanced RISC Machines, 高级的 RISC 机器)：
            RISC(Reduced Instruction-Set Computer 精简指令集计算机)
            
            ARM 体系结构在移动市场上占据主导地位，大多数智能手机，平板电脑都是基于 ARM 处理器。
            
        c. AVR 体系简介（Atmel 公司）：
        
            主要用于非常低端的嵌入式系统中。
                
    1.5 公制计量单位：
    
        kb : 2^10 字节
        MB : 2^20
        GB : 2^30
        TB : 2^40
    
        1kbps : 通信线的传输速度是1000位/秒 10^3
        1Mbps: 局域网的速度是 1 000 000 位/秒 10^6
        1Gbps 10^9  位/秒
        1Tbps 10^12 位/秒       
           
    1.6 本书概览:
    
        本书详细讨论 数据逻辑层、微体系结构层、指令系统层和操作系统层。
        
        第二章：介绍计算机组件组成部分--处理器、存储器、输入/输出设备
        
        第三章：数字逻辑层，是真正的计算机
        
        第四章： 微体系结构层及其控制电路
        
        第五章：指令系统层
        
        第六章：操作系统层的一些指令、内存组织和控制机制。
        
        第七章：汇编语言层
        
        第八章：并行计算机
        


"-----------------------------------------------------------"

                        第二章 计算机系统组成

    数字计算机是由处理器、寄存器和输入/输出设备组成的内部互联系统。
 
    2.1 处理器
        
        中央处理部件(Central Processing Unit, CPU) 可以说是计算机的“大脑”，
        其功能是通过从主存储器中逐条进行取指令、分析指令、和执行指令的过程来执行计算机程序。
        
        计算机的各组成部件通过总线连接在一起，总线是一些平行导线的集合，计算机用它来传递地址、
        数据和控制信号。它可以在 CPU 之外，连接 CPU、存储器以及输入/输出设备；也可以在 CPU
        内部连接 CPU 的各组成部分。现代计算机中有多条总线。
        
        CPU 由相对独立的几个部分组成：
        
            a. 控制器：
            
                控制器负责从主存储器中取指令和分析指令类型
                
            b. 算术逻辑部件：
                
                算术逻辑部件通过完成诸如加法、逻辑与等算术逻辑运算来执行指令。
                
            c. 存储器：
            
                CPU 内部包含一个小容量、高速度的存储器，用来存放中间结果和一些控制信息。
                这个存储器由多个寄存器组成，每个寄存器都有确定的存储容量和相应的功能。
                每个寄存器可以存放不超过其存储范围的数，它们本身就在 CPU 内部，可以被 CPU 高速读写。
                
                CPU 中最重要的寄存器是程序计数器（Program Counter，PC）, 它指向下一条将被取出
                用于指向的指令。（程序计数器的名称有时候容易引起误解，因为实际上它并没有任何计数的作用
                而是一个约定俗称的叫法）
                
                同样重要的寄存器还有指令寄存器（Instruction Register, IR）其中存放着当前正执行的指令。
                多数计算机中还有许多其他的寄存器，一些是通用，一些事专用寄存器。
                还有一些寄存器被操作系统用于控制计算机。
                
        2.1.1 CPU 组成：
    
            我们把寄存器(一般来说为 1 ~ 32个)、算术逻辑部(ALU)和连接它们的内部总线叫做数据通路。
            寄存器给 ALU 的两个输入暂存器提供输入，暂存器的功能是在 ALU 进行计算时维持 ALU 的输入数据。
            数据通路在所有的计算机中都非常重要。
            
            ALU 本身对输入数据进行加、减等简单运算，然后将产生的运算结果送入输出暂存器，经输出暂存器存回某个寄存器中。
            以后在需要时还可以从寄存器写入（也就是保持）到主存中。并非所有的 CPU 中都有输入或输出暂存器。
            
            大多数指令可以归并到下面两类当中：寄存器 - 主存指令 或 寄存器 - 寄存器指令。
            寄存器-主存指令用于寄存器和主存之间交换数据。例如：将主存当中的字取到寄存器当中供后续指令使用。
            （"字"：是主存和寄存器间交换数据的单位，一个字可以是一个整数）也可以是将寄存器中的数据存回主存。
            
            另一类指令是寄存器-寄存器指令。典型的寄存器-寄存器指令从寄存器中取得两个操作数，送入 ALU 的
            输入暂存器，对它们进行运算，然后再将运算结果送回到其中的一个寄存器当中。
            
            ALU 将两个数据进行运算并将结果写回的过程称为数据通路周期，这是大多数 CPU 的核心。从某种意义上
            来说，它决定了计算机的功能。现在计算机中有多个可并行操作的 ALU， 有些是为不同的功能专门设计的。
            数据通路周期越快，计算机运行起来就越快。
            
        2.1.2 指令执行：
    
               一般来说，计算机执行一条指令的过程可以大致分为以下几个步骤：
               
               a. 从主存中取下一条指令到指令寄存器中。
               
               b. 将程序计数器指向后面的一条指令。
               
               c. 判断刚取得的指令的类型。
               
               d. 若该指令用于到某个主存单元，则对该主存单元进行寻址。
               
               e. 必要时， 从主存中取一个字到 CPU 的寄存器中。
               
               f. 执行指令。
               
               g. 返回第 a 步准备执行下一条指令。
                
               这个过通常称为 取指-- 译码 -- 执行周期，是多有计算机操作的核心。
               
            能用程序来模拟 CPU 的功能这个事实正说明了程序并不一定需要由一大堆电子器件组成的“硬件”CPU 来执行。
            实际上，可以由另一个程序通过取指令、分析指令和执行指令的过程来执行一个程序的指令。
            这个获取、分析执行其他程序指令的程序就是解释器。
            
            硬处理器和解释器之间的等价性对计算机组成和计算机系统设计有很大影响。
            在确定一台新计算机的机器语言 L 后，设计小组要决定的事情就是研制一个硬处理器来直接执行 L 的程序
            还是写一个解释器来解释执行 L 的程序。当然，选择写解释器也需要由硬件来执行解释器。
            一部分指令由硬件执行，另一部分由软件解释的混合方案也是可行的。
            
            解释器将目标机的指令分解成几个更小的步骤执行，这就使得运行解释器的计算机比起运用硬件来实现的目标机
            来说更简单也更廉价一些。当目标机的指令集比较庞大，或是指令集中有许多复杂指令的选项时，这种节约
            就比较可观，本质上的原因是硬件被软件（解释器）所取带，而用硬件实现同样的功能比用软件实现开销要大许多。
            
            早期计算机的指令集比较小，指令比较简单，但是人们对更强大计算机的需求，导致这就要求每条
            指令功能更强一些，人们发现，虽然复杂的指令单条执行时间要长一些，但却能使整个程序执行更快。
            有时，为简化程序，也常常把两条经常连续执行的指令合并成一条指令。
            
            复杂一些的指令整体性能更好的原因就在于单步操作的执行有时可以重叠，或用不用的硬件并行执行。
            
            到20世纪50年代，IBM 认识到，支持一个多有计算机都执行相同指令的计算机系列对于IBM和客户来说
            有很大优势，IBM 引入体系结构这个术语来描述中一层次的兼容性。他们希望同系列的计算机的系统结构
            相同，但用不同的实现方法来执行同样的程序，只在价格和速度上各不相同。但如何使得低成本的计算机
            能执行高价、高性能计算机的所有复杂指令呢？
            
            实现的途径就是解释。 Maurice Wilkes 在 1951 年投产的技术是设计简单、价格低廉的计算机同样
            可以执行许多指令。由它产生了 IBM System/360 体系结构的一些列兼容计算机，全系列在性能和价格
            两方面都跨越两个数量级，直接由硬件（不通过解释）实现的是最昂贵的。
            
            解释执行指令的简单计算机有许多其他的优势，其中最重要的有：
                a. 在解释过程中改正指令实现中的错误，甚至补偿基础硬件中的设计缺陷。
                
                b. 可以以最小的代价增加新指令，甚至在计算机发货后也能做到这个点。
                
                c. 结构化设计，可以对复杂指令方便地进行升级，测试和文档化。
                
            随机底层半导体技术的进步，20世纪70年代，基于解释器的体系机构成为主流的设计计算机的手段。
            运行解释器的简单处理器已经占有统治地位。解释器的运用降低了实现复杂指令的固有成本，因此
            设计者开发更多的复杂指令，特别是增加了对指令操作数进行寻址的方式。
            
            到来 70 年代后期，甚至微处理器也换成了基于解释器的设计，当时，微处理器设计者面对的主要挑战
            是通过集成电路来解决处理器不断增长的复杂性。基于解释器方案的优点是指需要设计一个简单的处理器
            而把复杂性问题大部分转移到存放解释器的存储器中，即把复杂的硬件设计转为复杂的软件设计。
            
            当时使得解释器大行其道的另一个原因是用来存放解释程序的快速只读存储器-控制存储器。
            控制存储器的存在使得指令的执行大大提高。
            
        2.1.3 RISC 和 CISC:
            
            RISC: 是精简指令计算机(Reduced Instruction Set Computer)的缩写，这种计算机特点时指令集
            相对较小，一般为50条指令左右。和代表复杂指令计算机的CISC(Complex Instruction Set Computer)
            相对应。
            
            RISC 这种新的处理器和当时的商业处理器有着很大的区别，新CPU 不存在兼容过去产品的问题，
            他们的设计者可以自由的选择新的指令集，来最大限度提高系统的整体性能。由于设计之初就强调
            选用能快速执行的简单指令，设计出能够快速启动的指令是提供性能的关键，每秒启动的指令的条数
            比单条指令的执行时间更重要。
            
            一场 RISC 的支持者攻击已经建立好的秩序（VAX，Intel和IBM大型机）的宗教式战争就打响了。
            他们宣称设计计算机的最好途径是选择少量能在数据通路的一个周期内执行的简单指令，其过程
            为取两个寄存器的值，以某种方式进行运算，再将结果存回寄存器。他们认为，即使RISC 需要
            四或五条指令来完成 CISC 的一条指令的功能，但如果 RISC 指令能比 CISC 快 10 倍的话，
            那么 RISC 还是占优。此时，主存的速度已经赶上了只读控制存储器的速度，解释器的执行代价已经高了许多，
            这对 RISC 相当有利。
            
            也许有人想，以RISC 技术在性能上的优势，RISC机(SUN 的 UltraSPARC)应该已经在市场上横扫 
            CISC机(intel Pentium)了，但事实并非如此，为什么呢？
                首先，一个向后兼容的问题，其次，Intel 在其 CISC 体系结构中也采用了 RISC 思想，从 486 开始，
                Intel 的 CPU 中就包含能在单个数据通路周期中执行一些最简单（最常用的）的指令的 RISC 核心，
                而还是用原有的 CISC 方式解释执行那些复杂的指令。
                
        2.1.4 现代计算机设计原则:
        
            第一台 RISC 机诞生到现在，在当前硬件技术条件下，许多设计原则已经被人们作为好的设计计算机原则接受了。
            确实存在一些设计原则，有时也说是 RISC 机设计原则，下面讨论几条：
            
                a. 所有指令由硬件直接执行：
                
                    不要再由微指令解释一遍。减少一层解释可提高多数指令的执行速度。对于实现 CISC 指令集的计算机
                    则可将较复杂的指令分解成可被一段微指令执行的单独部分，虽然这个额外的步骤会减慢计算机的速度，
                    但对于使用额度不高的指令，还是可以接受的。
                    
                b. 最大限度提高指令启动速度：
                
                    现代计算机采取了许多策略来尽可能提高性能，最主要的是在每秒中内启动尽量多的指令。
                    毕竟，如果能在每秒钟启动 5 亿条指令，那就是一个 500-MIPS 的处理器，而不必计较
                    这些指令实际用了多长时间执行完。（Millions Instructions Per Second,MIPS,即每秒
                    百万条指令。MIPS处理器的中也是字头组成的缩写略语，但它代表的是 Microprocessor without
                     InterLocked Pipeline Stage，即无内部互锁流水级的微处理器）这条原则意味着指令的并行处理
                     对提高性能起着重要的作用。
                     通过同时执行多条指令是有可能提高性能的，但要做许多记录来保证指令的正确执行顺序。
                     
                c. 指令应容易译码：
                
                    严重阻碍指令启动速度提高的因素之一是判断指令要用到的资源的译码过程，应采取一切手段来加速指令译码。
                   
                d. 只允许读写主存指令访问主存：
                
                    将计算机操作分解为独立步骤的最简单办法之一是规定多数指令的操作数来自 CPU 的寄存器中
                    且结果写回到寄存器中，把将操作数从主存读入寄存器的操作交给单独的指令来完成。
                    最好是只有 LOAD 和 STORE 指令可以访问主存，所有其他的指令操作数应只在寄存器中。
                    
                e. 提高足够的寄存器：
                
                    由于访问主存相对来说较慢，这就要提供多一些寄存器，保证从主存中去一个字后，就能一直
                    保存到寄存器中。
                    
        2.1.5 指令级并行:
        
            计算机设计者一直为提高他们设计的计算机的性能而努力着，通过提高芯片的主频来使它运行的快一些
            是一条途径，单每个新设计都有在当时条件下主频能提高到的极限，因此，多数设计者把并行处理作为
            在给定主频下取得更好性能的另一条途径。
            
            一般来说有两种形式：指令级并行和处理器并行。
            前者指在指令之间应用并行，使计算机在单位时间里处理更多的指令。后者是指多个CPU一起工作，
            解决同一个问题。两者都有自己的优点。
            
            a. 指令流水：
                
                实际上，预取把指令执行分解成两个部分：取指令和实际执行指令。
                指令流水的概念把这个策略再往前一步，它一般把指令执行分解成更多的部分，每个部分由
                精心设计的硬件分别执行，都可以并行运行。
                
                下面将指令执行分解为5个部分：也可以说是 5 个子过程的流水过程：
                取值单元、指令译码单元、取操作数单元、指令执行单元、写回单元
                
                指令流水既可以用指令时延（执行一条指令的时间），也可以用处理器带宽（CPU 的 MIPS 数）来衡量。
                对于有 n 个子过程，时钟周期为 T 纳秒的流水线，其指令时延为 nT 纳秒，因为每条指令需要 n 个
                步骤才能执行完毕，每个步骤需要用时 T 纳秒。
                
            b. 超标量体系结构：
            
                “超标量”的定义发展又点快，现在通常用来描述那些可以在一个时钟周期内启动多条通常是 4 或 6条--指令处理器。
                超标量 CPU 必须有多个功能部件来处理这些指令。
                
        2.1.6 处理器级并行：
        
            指令级并行对提高速度有所帮助，但流水线和超标量体系结构提高的速度很难超过 5 倍或10倍
            想要提高50倍，100倍提高速度，唯一办法是设计多 CPU 的计算机。
            
            a. 数据并行计算机：
                
                单指令流多数据流处理器，称为 SIMD处理器由许多在不同数据集合上执行同样指令序列的完全相同的处理器组成。
                
            b. 多处理器：
                
                由于多个高速处理器通过同一条总线上频繁访问内存，将造成总线上的冲突。
                多处理器系统中紧耦合的cpu相对应。
                
            c. 多计算机：
            
                多计算机，没有公共内存，有各自的私有内存，这种系统称为多计算机系统。
                多计算机系统中的 CPU 有时也称为 松耦合。     
                    
    2.2 主存储器：
        
        存储器是计算机用来存放程序和数据的地方，storage 这个词越来越特指磁盘存储器。
        
        2.2.1 存储位：
        
            存储的最基本单元是二进制数，即二进制位。
            
        2.2.2 内存编址：
        
            存储器由许多可存放可存放一段信息的单元（或位置）组成，每个单元有一个编号
            程序可以通过这个编号来访问这个单元，这个编号就是这个单元的地址。
            若存储器有 n 个单元，它们的地址就是 0 ~ n-1。
            存储器中所有的单元包含的位数相同。如果一个单元包含了 k 位，那么该
            单元可以存放 2^k 个不同的位组合的一种。
            
            使用二进制数的计算机的存储器地址也用二进制数表示，若地址为 m 位，则可编址
            的最大单元位数是2^m。
            
            地址位数有存储器中需要直接编址的单元的最大位数目决定，而与每个单元有多少
            存储位无关。具有 2^12 个存储单位的 8 存储器和具有 2^12 个存储单元的 64
            位存储器所需的地址位都是 12 位。
            
            存储单元最重要的特性是它最小的可编制单位，近年来，几乎所有的制造商都标准
            化为 8 位存储单元，即字节，有时也称为 八位组。字节再组合成字。
            
            32位字的计算机每个字有 4 个字节，64位字的计算机每个字有 8 个字节。
            大多数计算机指令都是对字进行操作。
            
            
        2.2.3 字节顺序：
        
            每个字中的字节地址可以从左到右或从右到左编排。
            从左到右的排法称大端派计算机。
            从右到左的排法称小端派计算机。
            
            在计算机之间交换数据时，如果在字节编址顺序上没有一个统一的标准，总是一件麻烦事。
            
        
        2.2.4 纠错码：
        
            由于电源线的尖峰电压或其他原因，计算机主存偶尔也会出错。为防止这些错误，
            这些主存采用检错码或纠错码，即往每个主存字中按特别的规定加上一些附加位，
            当从主存中读出一个字时，用这些附加位来校验主存是否出错。
            
        2.2.5 高速缓存：
        
            CPU 的设计者利用实现流水和超标量运算，使CPU变得更快，
            存储器的设计者使用技术使得提高芯片的容量，而不是速度，
            这就使两者的速度差别越来越大。这种不平衡表现在CPU发出
            访问存储器请求后，要经过多个 CPU 周期才能读到存储器内容。
            
            有两种办法解决这个问题：
                1、遇到访问存储器的指令时，就启动读存储器的动作，然后继续执行下面的指令。
                    如果在存储器的内容还没有读出时遇到要使用这些内存字的指令就使CPU 暂停下来等待，
                    存储器越慢，带来的损失越大。
                2、 另一种办法，不让CPU 暂停执行，而要求编译器在读到内容之前不要生成使用
                    该内容的指令。实际上这是软件暂停代替了硬件暂停，对性能的影响是一样的。
                    
            高速缓存：小容量高速存储器称为高速缓存。
            高速缓存的工作原理十分简单：把读取频率最高的存储器内容保存在高速缓存中。
            CPU 先去高速缓存中查找，查找不到再去主存器中查询。
            
            高速缓存的成功和失败就取决于将那些内容保存到高速缓存中，今年来，人们已经认识
            到程序并不是随机访问存储器。如果CPU访问存储器地址A,那么它的下一次对存储器的
            访问一般来说是临近 A 的，程序本身就是这样的例子。除分支转移和过程调用外，
            指令时从相邻的存储器单元中取出的。
            
            即在段时间，cup对存储器的访问总是局现在整个存储器的一小部分，这称为局部性原理。
            它构成所有高速存储系统的基础，总的思路是访问存储的某个字后，将该字之后和它的相邻
            单元从低速的大容器存储器系统中取到高速缓存中。
             
        2.2.6 内存封装及其类型：
        
            从半导体存储器诞生起，到20世纪90年代早期内存都是以芯片为单位制造、销售和安装。
            从20世纪90年代，内存封装采用新方法，通常是将一组芯片、一般是 8 或16片一起安装
            在一块印刷电路板上出售，根据电路板上的连线是一面还是双面，一般把内存条分为
            单面接头内存条（Single Inline Memory Module, SIMM）和双面接头内存条
            （Dual Inline Memory Module DIMM）,主要取决于板子的单面或双面是否有一排接线。
            DIMM 通常在电路板双面都有接线，每面有120根接线，每个时钟周期传输64位数据。
            典型的DIMM内存条上有 8 块容量为 256MB的芯片，整个内存条容量为 2GB。
            笔记本计算机使用到双面接头内存条，体积比较小，叫作小型DIMM.
                  
    2.3 辅助存储器：
    
        2.3.1 层次存储结构：
            计算机采用层次存储结构：
            CPU中的寄存器、其存取速度可以满足 CPU 要求。
            高速缓存，目前的存储容量在32KB到几MB之间。
            主存储器：容量从 1GB 到几百G（内存条）
            固体硬盘和磁盘存取，当前永久存放数据的主力存储介质。
            备用存储磁带和光盘存储。
        
        2.3.2 磁盘：
        
            磁盘是由一个或多个表面涂有磁性材料的铝质浅盘组成，早期铝直径到50CM
            现在一般为 3 ~ 9cm,笔记本计算机磁盘直径在3CM以下。
            内含一个引导线圈的磁头正好浮在磁盘表面，中间隔着薄薄的一层空气垫。
            当磁头中有正和负电流通过时，就可磁化磁头正下方的磁盘表面，根据
            电流的极性不同，是磁性颗粒朝左或朝右偏移。
            而当磁头通过磁性区域时，将被感应出正电流或负电流，这样，就读出了存在磁盘
            上是数据位，是磁盘在磁头下面旋转，数据位就可以被写入磁盘或从磁盘中读出。
            
            磁道：磁盘旋转一周后写入磁盘的数据位形成的环行为磁道。
            每个磁道可划分为固定长度的扇区，每个扇区一般可以存放 512 字节的数据区，
            数据区前有用于在读写这前对磁头进行同步的前导区，数据区之后是纠错码，
            连接两个扇区的是隔离带。
            
            所有磁盘都有一个能从磁盘旋转的轴心伸缩到不同半径的磁盘臂，磁盘臂能伸展到
            的每个半径对于着一个不同的磁道，这样，磁道就是围绕着轴心的一系列同心圆。
            磁道的宽度取决于磁头大小和磁头轴向伸缩的精确度。
            在现在技术下，每厘米磁盘可以有 50000个磁道，即磁道宽度为200纳米。
            磁道在物理上并不是磁盘表面的凹槽，而只是简单有磁性材料组成的圆环，
            中间有隔离区将磁道和它里面的磁道和外面的磁道隔离开。
            
            磁盘的性能取决于许多因素，要读写扇面，首先需将磁盘臂轴向移动到相应半径的位置，称为寻道。
            磁盘的平均寻道时间为 5 ~ 10 毫秒。
            
            与磁盘驱动器相连的是控制它的芯片--磁盘控制器，有些控制器中甚至有一个完整的CPU.
            控制器的任务包括接受软件发出的READ、WRITE和FORMAT等命令，控制磁盘臂的运动，发现和纠正错误。
            将从内存中读出的 8 位字节转换为位串写到磁盘中。或相反，将磁盘中读出的位串转换8位字节数据。
        
        2.3.3 固盘：
        
            作为传统磁盘技术的替代品，由高速且非易失的 Flash 存储器制造的固盘(Solid-State Disk,SSD,也成固态盘)
            Flash 硬盘由许多固态 Flash 存储单元组成，而 Flash 存储单元由一个特制的 Flash 晶体管制成。
            嵌在晶体管内部的是一个浮空栅极，可通过高电压对其充电或放电。Flash 存储单元就是一个简单的晶体管。
            为给 Flash 存储单元编程（写入数据），需要在其控制极上加载高电平（12V）,这样，会加速热载流子
            注入浮空栅极个过程，电子嵌入浮空栅极上，在Falsh 晶体管内部加载了负电荷。这些嵌入的负电荷会
            使将晶体管打通所需要的电压增大，这样，通过在高或低的控制电压下检测晶体管是否导通，就有可能
            判断浮空栅极是否充了电荷，由此可判断存储单元存放时 “0” 还是 “1”，即使外部电压从系统中撤出
            嵌入在晶体管中的电荷也会一直存在，从而保证 Flash 存储单元中信息是非易失性的。
            
            SSD 本质上属于半导体存储器，因此性能超过旋转磁盘很多，寻道时间为0.
            与磁盘相比，价格贵，使用寿命短，典型的Flash 存储单元可写 100 000 次。               
            
    2.4 输入/输出 设备：
    
        2.4.1 总线：
            每个输入/输出设备都有两部分组成，一部分为 I/O 控制器，包括绝大多数的接口电路；
            另一部分为设备本身，入磁盘驱动器等。I/O控制器通常直接集成在主板中，有时也以插卡的
            形式插在主板的空槽里。
            
            I/O 控制器的任务是控制其输入/输出设备和处理总线上的访问信号。
            I/O 控制器不用CPU干涉就能完成对内存的读写，这种方式称为直接访问内存（Direct Memory Access）DMA.
            总线不仅用来供 I/O 控制器使用，还被 CPU 用来取指令和数据。
            如果CPU 和 I/O 控制器需要同时使用总线的话，这将由一片名为总线仲裁器的芯片决定由谁来使用。
            一般I/O 设备优先级高于 CPU。
            
            PCI 和 PCIe 总线：
                PCI 总线，有Intel 设计，CPU 通过高速连线和存储控制器相连，该控制器再通过 PCI 总线和内存直接相连。
                PCIe 甚至不能成为总线，它是采用串行传输和包交换的点到点网络，传输方式接近互联网，而不像传统总线。
                

"-----------------------------------------------------------"
                                
                       第三章 数字逻辑层         

"-----------------------------------------------------------"


"-----------------------------------------------------------"

                      第四章 微体系结构层

"-----------------------------------------------------------"


"-----------------------------------------------------------"

                     第五章 指令系统层
                      
"-----------------------------------------------------------"

"-----------------------------------------------------------"

                    第六章 操作系统层
                    
"-----------------------------------------------------------"


"-----------------------------------------------------------"
                        
                   第七章 汇编语言层     

"-----------------------------------------------------------"

"-----------------------------------------------------------"
                
                    第八章 并行计算机系统结构            
    
"-----------------------------------------------------------"