"-------------------------------------------------------------------"
            
            学习极客时间王争老师《数据结构和算法之美》学习笔记   
            
"--------------------------------------------------------------------"

"""01: 为什么要学习数据结构和算法 """

    程序员所谓的瓶颈期其实是你自己是不是跟着行业在发展，还是每个项目都在重复
    的堆砌业务逻辑，没有难度递进，没有任何提升，十年和一年也没有什么区别。
    
    我们学习数据结构和算法，并不是为了死记硬背几个知识点，我们的目的是建立时间复杂度、
    空间复杂度意识、写出高质量的代码、能够设计基础架构、提升编程技能，训练逻辑思维。


"""02: 如何抓住重点，系统高效地学习数据结构和算法 """
    
    1、什么是数据结构和算法：
    
        从广义上讲：
        
            数据结构就是指一组数据的存储结构，算法就是操作数据的一组方法。
        
        从狭义上讲：
            
            指某些著名的数据结构和算法，比如，队列、栈、队、二分查找、动态规划等。
            
    2、数据结构和算法什么关系：
    
        数据结构和算法是相辅相成的，数据结构是为算法服务的，算法要作用在特定的数据结构之上。
        
    3、学习的重点在什么地方：
    
        学习数据结构和算法，首先要掌握一个数据结构和算法中最重要的概念--- 复杂度分析。
        
        10 个数据结构：
        
            数组、链表、栈、队列、三列表、二叉树、堆、跳表、图、Trie 树
            
        10 中算法：
        
            递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符匹配算法。
            
    4、小结：
    
        知识需要沉淀、不要试图一下子掌握所有。
        学习知识的过程是反复迭代，不断沉淀的过程。
        边学边练、多闻多思
        
        
"""03:  复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗"""

    数据结构和算法本身解决的是 "快" 和 "省" 的问题，及如何让代码运行得更快，如何让代码更省存储空间。
    所以，执行效率是算法一个非常重要的考量指标。
    
    复杂度分析是整个算法学习的精髓，只要掌握它，数据结构和算法的内容就基本上掌握了一半。
    
    1、为什么需要复杂度分析:
        
        摆脱一些客观因素的影响，我们需要一个不用具体的测试数据来测试，就可以粗略的估计算法
        的执行效率的方法。就是我们今天要讲的时间、空间复杂度分析方法。
        
    2、大 O 复杂度表示法：

        所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。
        
        我们可以把这个规律总结成一个公式：
         
            T(n) = O(f(n))
        
            其中： T(n) : 是代码执行的时间
                   n   : 表示数据规模的大小
                  f(n) : 表示每行代码执行的次数总和。
                  O    : 表示代码执行时间 T(n) 与 f(n) 表达式成正比。
            
            大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随
            数据规模增长的变化趋势，所以，也叫作渐进时间复杂度，简称"时间复杂度"。
   
    3、 时间复杂度分析：
           
        （1） 只关注循环执行次数最多的一段代码
            大 O 这种复杂度表示方法只是一种变化趋势，我们通常会忽略掉公式中的常量、低阶和系数
            只需要记录一个最大阶的量级就可以了，所以，我们在分析一个算法、一段代码的时间复杂度的时候
            也只关注循环执行次数最多的那段代码就可以了。
            
        （2） 加法法则：总复杂度等于量级最大的那段代码的复杂度。
       
        （3） 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
        
    4、几种常见时间复杂度实例分析：
    
        常量阶： O(1)
            
            O(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。
            比如：下面代码有三行，他的时间复杂度为O(1) 而不是O(3)
                int i = 8
                int j = 6
                int sum = i + j
            
            只要代码执行时间不随 n 的增大而增大，这样的时间复杂度都记作 O(1).
            或者，一般情况下，只要算法中不存在循环语句、递归语句、即使有上千万行的代码
            其中的时间复杂度也是O(1)。
       
        对数阶： O(logN)、O(nlogn):
            
            如下面一段代码片段：
            
                i = 1;
                while (i <= n){
                    i = 2 * i
                }
                
            第三行代码是循环执行次数最多的，所以，我们只要计算出这行代码被执行了多少次，就能知道
            整段代码的时间复杂度。
            从代码可以看出，变量 i 的值从 1 开始取，每循环一次乘以 2。当大于 n 时，结束循环。
            2^x = n 求解 x 的值就是这段代码执行的次数。 x = log2n，所以代码的时间复杂度是O(log2n)
                
                i = 1
                while (i <= n){
                    i = i * 3
                }    
            上面这段代码的时间复杂度是 O(log3N)
           
            实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们都可以记为 O(logn)
                log3N = log32 * log2N,所以 O(log3N)=O(C * log2N) 其中 c = log32 是个常量。
            因此在对数阶时间复杂度的表示法我们忽略"底"，统一为 O(logN)
            
            如果一段代码的时间复杂度是 O(logN),我们循环执行 n 遍，时间复杂度就是 O(nlogN)了。
            快速排序和归并排序时间复杂度都是 O(nlogN)
               
        3、O(m + n)、O(m*n)
            
            有两个数据规模的复杂度。
            
            int cal(int m, int n){
                
                int sum_1 = 0;
                int i = 1;
                for(; i < m; ++i){
                    sum_1 = sum1 + i
                }
                int sum_2 = 0;
                int j = 1;
                for(; j < n; ++j){
                    sum_2 = sum_2 + j;
                }
                return sum_1 + sum_2;    
            }
            
            从代码看出，m 和 n 是代表两个数据规模，我们无法先评估 m 和 n 谁的量级大，所以我们
            表示复杂度的时候，就不能简单地利用加法法则，省略其中的一个。
            上面代码的时间复杂度是 O(m + n)
    
        线性阶： O(n)
        平方阶、立方阶、。。K次方阶：O(n^2)、O(n^3)、O(n^K)
        指数阶：O(2^n)
        阶乘阶：O(n!)
        
    5、 空间复杂度分析：
    
        时间复杂度的全称是：渐进时间复杂度，表示算法的执行时间与数据规之一个之间的增长关系。
        空间复杂度的全称是：渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系。
        
        void print(int n){
            int i = 0;
            int [] a = new int[n];
            for(i; i<n;i++){
                a[i] = i * i
            }
            
            for(i = n-1; i>=0; --i){
                print out a[i]
            }
        }
        
        跟时间复杂度一样，第二行代码中，我们申请了一个空间存储变量 i,但是它是常量阶的，跟数据规模 n 没有关系。
        所以我们可以忽略。第三行申请一个大小为 n 的 int 类型数组，除此之外，剩下的代码里没有占用更多空间，
        多以整段代码的空间复杂度是 O(n)
        我们常见的空间复杂度是 O(1), O(n), O(n^2), 像 O(logN),O(nlogN)这样对数阶的复杂度平时用不到。
        
    6、小结：
    
        复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系。
        
        
"""04: 复杂度分析（下）： 浅析最坏、最好、平均、均摊时间复杂度"""        
        
    1、最好、最坏情况时间复杂度：（best/worst case time complexity）
        
        # n 表示数组 array 的长度
        int find(int [] array, int n, int x){
            int i = 0;
            int pos = -1;
            for(; i<n; ++i){
                if (array[i] == x) pos = i;
            }
            return pos;
        }
        
        # n 表示数组 array 的长度
        
        int find(int[] arrar, int n, int x){
            int i = 0;
            int pos = -1;
            for(; i < n; ++i){
                if (array[i] == x){
                    pos = i;
                    break;
                }
            }
            return pos;
        }       
            
        因为，要查找的变量 x 可能出现在数组的任意位置。如果数组中第一个元素正好是要查询的变量 x
        那么就不需要遍历剩下的 n - 1 个数据了，那时间复杂度就是O(1), 如果数组中不存在，就需要
        把整个数组遍历一遍，时间复杂度就是 O(n)。不同情况，时间复杂度不同。
        
        为了表示代码在不同情况下不同时间复杂度，我们需要引起三个概念：最好情况时间复杂度、最坏情况时间复杂度和
        平均情况时间复杂度。
        
        平均情况时间复杂度:
            最好情况和最坏情况时间复杂度都是极端情况下的代码复杂度，发生的概率并不大。为了更好的表示
            平均情况下的复杂度，我们引入了另一个概念：平均情况复杂度，简称平均复杂度。
            
            要查找变量 x 在数组中的位置， 有 n + 1 种情况：在数组的 0 ~ n-1 位置中和不在数组中。
            我们把每种情况，查找需要遍历的元素个数累加起来，然后再除以 n + 1，就可以得到需要的元素个数
            的平均值 (1+2+3+...+n+n)/(n+1) = n(n+3)/2(n+1) 时间复杂度的大 O 标记法中，可以省略掉系数
            低级和常量，因此得到的时间复杂度是 O(n)
    
    2、均摊时间复杂度：
    
        # array 表示一个长度为 n 的数组
        # 代码中的 array.length 就等于 n
        
        int[] array = new int[n];
        int count = 0;
        void insert(int val){
            if (count == array.length){
                int sum = 0;
                for(int i=0; i< array.length; ++i){
                    sum = sum + array[i];
                }
                array[0] = sum;
                count = 1;
            }
            array[count]= val;
            ++count;
        }
        
        均摊时间复杂度就是一种特殊的平均时间复杂度。

"""数组：为什么很多编程语言中数组都是从0开始编号"""

    1、如何实现随机访问？
        
        数组（array）：是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。
        线性表：就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。
                除了数组，链表、队列、栈等也是线性表结构。
                而与线性表对立的概念是非线性表，比如二叉树、堆、图等，在非线性表中，数据之间并不是简单的前后关系。
        
        连续的内存空间和相同类型的数据，正是因为这两个限制，它才有一个“杀手锏”的特性：“随机访问”。
        但是有利有弊，这两个限制也让数组的很多操作变得低效，比如插入和删除。
        
        计算机给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问
        数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素的内存地址：
        
                a[i]_address = base_address + i * date_type_size
                
                其中：base_address 内存块首地址
                     data_type_size 存储数据类型的大小
                     i  数组中的第几个数据
        
        数组是适合查找操作，但是查找的时间复杂度并不为 O(1)，即便是排好序的数组，采用二分法
        时间复杂度也是 O(logn),数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。
        
    2、低效的插入和删除：
    
        插入操作：
        
            假设数组的长度为 n , 如果数组在末尾插入时间复杂度为 O(1),
            在数组的开头插入时间复杂度为O(n),平均时间复杂度为 O(n).
            如果数组是有序的，我们在某个位置插入一个新的元素，就必须
            搬移 k 之后的数据。但是如果数组中的数据并没有任何规律，数组只是被当做一个存储
            数据的集合，在这种情况下，我们可以避免搬迁数据，直接将第 k 位的数据搬迁到数组
            末尾，将新的元素放入到第k个位置。
        
        删除操作：
        
            如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬迁数据，不然中间
            会出现空洞，内存就不连续了。
            和插入类似，末尾删除最好情况时间复杂度O(1),删除头数据时间复杂度为O(n),平均也是O(n)。
            
    3、警惕数组的访问越界问题：
        在 c 语言中只要不是访问受限的内存，所有的内存空间都是可以访问的。
        
    4、容器能否完全替代数组：
    
        对于业务开发，直接使用容器就足够，省时省力，毕竟损耗一丢丢性能，完全不会影响到整个系统整体性能，
        但是，如果做非常底层的开发，比如网络框架，性能的优化需要做到极致，这个时候数组优于容器。
 
    5、为什么数组从 0 开始编号，而不是从 1 开始呢：
    
        从数组的存储的内存模型上看，“下标”最确切的定义应该是“偏移（offset）”
        如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，
        a[k] 就表示偏移为 k 个type_size 的位置，所以计算a[k]的内存地址只需要
        用给这个公式：
            a[k]_address = base_address + k * type_size
            
        如果从 1 开始编号，计算数组元素 a[k]的内存地址会变为：
            a[k]_address = base_address + (k - 1)*type_size
            
        对比两个公式，从 1 开始编号，你每次随机访问数组元素都会多了一次减法运算，
        对于 CPU来说，就是多了一次减法指令。
        
        数组作为分成基础的数据机构，通过下标随机访问数组又是非常基础的编程操作，
        效率的优化要做到极致，所以数组选择从 0 开始编号。
        
        二维数组的内存寻址方式：
        
            对于 m * n 的数组，a[i][j](i<m, j<n)的地址为：
            
             address = base_address + (i*n + j) * type_size
           
           
"""06 链表（上）：如何实现LRU缓存淘汰算法"""        
    
    缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有广泛的应用，比如 cpu 缓存、数据库缓存、浏览器缓存等。
    
    缓存的大小有限、当缓存被用满时，那些数据应该被清理出去，那些数据应该被保留？这需要缓存淘汰策略来决定。
    常见的三种策略：先进先出策略 FIFO(First In, First Out)、最少使用策略 LFU(Least Frequently Used)
    最近最少使用策略 LRU(Least Recently Used)。
    
    1、数组和链表：
    
        数组：一块连续的内存空间来存储，对内存要求比较高。
             数组的删除和插入操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是O(n)
             
            
        
        链表：通过“指针”将一组零散的内存块串联起来使用，我们把内存块称为“结点”。为了将所有的结点串联起来，
              每个结点上除了保持数据之外还有保持下一个结点的地址，我们把下一个记录下一个结点的地址的指针
              叫作后继指针 next。
              链表的插入和删除操作，只需要考虑相邻结点的指针改变，多对应的时间复杂度是O(1)
              链表要访问第 k 个元素，就没有数组那么高效，因为链表中的数据并非连续存储的，
              所以无法像数组那样，通过指针和下标，通过寻址公式就能计算出对于的内存地址，
              而链表需要一个结点一个结点地依次遍历，直到找到相应的结点。时间复杂度O(n)
              
        循环链表：
    
            循环链表的优点是从链尾到链头比较方便，当要处理的数据具有环形结构特点时，就特别适合采用循环链表。
        
        双向链表：          
              
            双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。双向链表比单向链表占用更大的内存空间。
            但是支持双向遍历，操作更灵活。
    
    2、单向链表和双向链表
            
        双向链表适合解决那些问题？
            
            从结构上看，双向链表可以支持 O(1)的时间复杂度的情况下找到前驱结点，这是这样的特点，也使得
            双向链表在某些情况下的插入、删除操作都比单链表简单、高效。
            单链表的插入和删除操作时间复杂度已经是 O(1)了，双向链表还能再怎么高效呢？
                
            删除操作：
                在实际的开发中，从链表中删除一个数据无外乎两种情况：
                    （1）删除结点中“值等于某个给定值”的结点
                    （2）删除给定指针执向的结点。
                        
                对于第一种情况，无论是单链表还是双链表，为了查找值等于给定的结点，都需要从头结点开始
                一个一个依次遍历对比，直到直到的值等于给定的值的结点，然后通过指针操作将其删除。
                尽管单纯的删除操作时间复杂度是 O(1),单遍历查找的时间是主要的耗时点，对应的时间复杂度是O(n)。
                    
                对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表
                不支持直接获取前驱结点，所以要找到前驱结点需要从头开始遍历，直到 p->next = q,说明 p 是 q 的前驱结点。
                但是对于双向链表这是明显的优势。
                第二种情况单链表删除的时间复杂度是 O(n),双向链表是 O(1)
                
            插入操作：    
                             
                同理，希望在链表的某个结点的某个指定结点前面插入一个结点，双向链表的时间复杂度是O(1)，单向链表是O(n)。    
                  
            查询操作：
                    
                对于一个有序的链表，双向链表的按值查询的效率也要比单链表更加高一些，因为，我们可以记录上一次查询的位置 p,
                每次查询时，根据要查找的值与 p 的大小，决定是往前查找还是往后查找。
                    
        在实际的开发中虽然双向链表更费内存，但是比单向链表应用更广泛，java 中的 LinkedHashMap 容器就是用的双向链表。
                    
    3、用空间换时间的设计思想：
    
        当内存空间充足的时候，如果我们更加追求代码的执行速度，我们可以选择空间复杂度相对较高，
        时间复杂度相对比较低的算法或数据结构，相反，如果内存比较紧缺，比如代码在手机或者单片机上
        这个时候，反过来要从时间换空间的设计思想。
        
        缓存实际上就会说利用空间换时间的设计思想，如果把数据存储在硬盘上，回比较节省内存，单每次查查比较
        慢，如果用缓存技术，事先把数据加载到内存中，虽然会比较消耗内存空间，但是每次数据查询的速度就大大提高。
        
        内存充足（时间换空间），内存不足（空间换时间）
        
    4、链表 VS 数组：
        
        数组和内存是两种截然不同的内存组织方式，正是因为内存存储的区别，他们插入、删除、随机访问操作的时间复杂度正好相反。
        
        不过数组和链表对比，并不能局限于时间复杂度。在实际开发中，不能仅仅利用时间复杂度分析来决定使用哪个数据结构。
        数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。
        链表在内存中不是连续存储所以对 CPU 不友好，没办法有效预读。
        
        数组的确定是大小固定，一经声明就要占用整块连续内存空间。如果声明数组过大，系统可能没有足够的内存空间分配给它，
        导致“内存不足（out of memory）”。声明过小，则可能出现不够用的情况，这时只能申请一个更大的内存空间，把原来数据
        拷贝进去，非常费时。
        链表本身没有大小的限制，天然支持动态扩容。
        
        如果你的代码对内存使用很苛刻，那数组更适合你，因为链表中的每个结点都需要消耗额外的存储空间去存储一份执向下一个结点
        的指针，所以内存消耗会翻倍，而且，对链表进行频繁的插入、删除操作，导致频繁的内存申请和释放，容易造成内存碎片
        如果是java 语言，就可能导致频繁的GC。
        
        实际开发中要根据具体情况，权衡究竟是选择数组还是链表。
        
    5、如何基于链表实现 LRU 缓存淘汰算法。
    
        我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。
        当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。
        （1）如果此数据已经被缓存在链表中了。我们遍历得到这个数据对应的结点，
             并将其从原来的位置删除，然后插入到表头。
        （2）如果此数据没有在缓存链表中，又可以分为两种情况：
            如果此时缓存未满，则将此节点直接插入到链表头。
            如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。
                
        现在缓存访问的时间复杂度为 O(n).
        可以引入散列表来记录每个数据的位置，将缓存的访问时间复杂度降到 O(1).
        
        
"""07 链表（下）：如何轻松的写出正确的链表代码"""   
    
    1、理解指针或引用的含义：
    
        将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针存储了
        这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。
        
    2、警惕指针丢失和内存泄露：
    
        如一个单链表，--->a--->b--->c--->d , 我们希望在结点 a 和相邻的结点 b 之间插入结点 x,
        假设当前指针 p 指向结点 a, 如果我们将代码实现成下面这个样子，就会发生指针丢失和内存泄露。
        
            （1）b->next = x; //将 p 的 next 指针指向 x 结点。
            （2）x->next = b->next; // 将 x 的结点的 next 指针指向 b 结点。
            
            p->next 指针在完成第一步操作之后，已经不再指向结点 b 了，而是指向结点 x .
            第二行代码相当于将 x 赋值给 x ->next,自己指向自己，因此，整个链表就断成两半。
            从结点 b 往后的多有结点都无法访问到。
            
        对于 c 语言，内存管理是由程序员负责，如果没有手动释放对应的内对应存空间，就会产生内存泄露。
        所以，我们插入结点时，一定要注意操作的顺序，同理，删除链表结点时，也一定要记得手动释放内存空间。
        
    3、利用哨兵简化实现难度：
    
        插入一个新结点:  
            new_node->next = p->next;
            p->next = new_node;
            
            如果是空链表上面的逻辑就不能用：
                if(head == null){
                    head = new_node
                }
            
        删除一个结点：
        
            p->next = p->next->next;
            
            如果我们删除的是最好一个结点，上面代码就不能使用：
                if(head->next == null){
                    head = null
                }
                
        针对链表的插入和删除操作，需要对插入第一个结点和删除的最后一个结点的情况进行特殊处理。
        
        还记得如何表示一个空链表吗？ head = null 表示链表中没有结点了，其中 head 表示头结点指针，指向链表的第一个结点。
        
        如果们引入哨兵结点，在任何时候不管链表是不是空，head 指针都会一直指向这个哨兵结点。
        我们也把也把有哨兵结点的链表叫作带头链表，相反，没有哨兵结点的链表叫作不带头链表。
        
        哨兵结点时不存储数据的，因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和
        删除其他结点，都可以统一为相同的代码实现逻辑。
                          
        代码一：
        
            //在数组 a 中，查找 key, 返回 key 所在是位置
            // 其中， n 表示数组 a 的长度。
            
            int find(char *a, int n, char key){
                
                // 边界处理，如果 a 为空，或者 n <=0,说明数组中没有数据，就不用进行下面的循环了。
                if(a == null || n <= 0){
                    return -1;
                }
                
                int i = 0;
                //这里有两个比较操作：i < n 和 a[i] = key
                while(i < n){
                    if(a[i] == key){
                        return i;
                    }
                    ++i;
                }
                return -1;
            }
            
            
        代码二：
        
            int find(char *a, int n, char key){
                if(a == null || n<=0){
                    return -1;
                }
                
                if(a[n-1] == key){
                    return n-1;
                }
                
                char tmp = a[n-1];
                a[n-1] =  key;
                int i = 0;
                while(a[i] ! = key){
                    ++i;
                }
                
                a[n-1] = temp;
                
                if(i == n-1){
                    return -1;
                }else{
                    return i;
                }
            }    
            
       对比两段代码，在字符串 a 很长的时候，比如 几十万，代码二执行更快，因为两段代码执行次数最多的是
       while 循环中的部分，第二段我们通过一个哨兵 a[n-1] = key,成功省略掉一个比较语句 i < n,
       不要小看这一条语句，当累计万次时，积累的时间很明显了。
       
    4、重点留意边界条件处理：
    
        用来检查链表代码是否正确的边界条件有这样几个：
        
            （1）如果链表为空时，代码是否能正常工作？
            （2）如果链表只包含一个结点时，代码是否能正常工作？
            （3）如果链表只包含两个结点时，代码是否能正常工作？
            （4）代码逻辑在处理头结点和尾结点时候，是否能正常工作。
            
    5、举例画图，辅助思考：
    
    6、多写多练，没有捷径：
    
        5个常见的链表操作
            单链表翻转
            链表中环的检测
            两个有序的链表合并
            删除链表倒数第n 个结点
            求链表的中间结点
       
        
    7、小结：
        
        写链表代码最考验逻辑思维能力。
     
     
"""08 栈：如何实现浏览器的前进和后退功能？""" 
    
    1、如何理解栈：
    
        栈：后进先出、先进后退
        从栈的操作特性上看：栈是一种“操作受限”的线性表，只允许一端插入和删除数据。
        
        相比数组和链表，栈给我带来了限制，并没有任何优势，那么我直接使用数组和链表好了，
        为什么还要使用“操作受限的栈”呢？
        
        事实上，从功能上说，数组或链表确实可以替代栈，但是，特定的数据结构是对特定场景的抽象，
        而且，数组和链表暴露太多的操作接口，操作上确实灵活，但是用时比较不可控，自然也就更
        容易出错。
        
        当某个数据集合只涉及从一端插入和删除数据，并且满足后进先出，先进后出的特性，我们就应该
        首先 “栈” 这种数据结构。
        
        实际上，栈既可以用数组实现，也可以用链表实现，用数组实现的叫顺序栈，用链表实现的叫链式栈。
        不管是顺序栈还是链式栈，我们存储数据只需要一个大小伟 n 的数组就够了。在入栈和出栈只需要
        一两个临时变量存储空间，所以空间复杂度是 O(1)
        
        注意：这里存数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n),因为，这个 n 个空间
        是必须的，无法省略掉，所以我们说空间复杂度时，是指除了原本的数据存储空间外，算法运行还需要
        额外的存储空间。
        
    2、栈在函数调用中的应用：
    
        操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数
        调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当函数执行完成，返回
        之后，将这个函数对于的栈帧出栈。
            
            int add(int x, int y){
                
                int sum = 0;
                sum = x+y；
                return sum;
            }
        
            int main(){
                
                int a = 1;
                int ret = 0;
                int res = 0;
                ret = add(3, 5);
                res = a + ret;
                pirntf("%d", res);
                return 0;
            }
   
    3、 栈在表达式求值中的应用：
        
        编译器如何利用栈实现表达式求值。
        比如 34 + 13 * 9 + 44 - 12/3 编译器是如何实现求值的呢？
        
        实际上，编译器是通过两个栈来实现的，其中一个保存操作数的栈，另一个是保存运算符的栈。
        我们从左到右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符
        栈的栈顶元素进行比较。
        
        如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或相同，
        从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入
        操作数栈，继续比较。
        
         
    4、栈在括号匹配中的应用：
    
        除了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。
        
        假设三种括号（）、{}、[]，并且可以任意嵌套。现在给三种括号的表达式字符串，如何检查是否合法？
        我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；
        如果遇到右括号，从栈顶取出一个括号，如果能够匹配，则继续扫描剩余的字符串。如果在扫描过程中
        遇到不能匹配的右括号，或栈中没有数据，则说明为非法格式。
        
        当所有的括号都扫描完成之后，如果栈为空，说明字符串为合法字符串；否则，有未匹配的左括号为非法字符串。
        
    5、浏览器如何实现前进、后退功能：
    
        我们使用两个栈 X 和 Y, 我们把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 栈中出栈，
        并将出栈的数据依次放入栈 Y 。当我们点击前进按钮时，我们依次从 Y 栈中取数据放入 x 栈中。当 x 栈中
        没有数据，那数名没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。    
        
    6、内容小结：
    
        栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。
        入栈和出栈的时间复杂度都是 O(1).       
   
    7、课后思考：
    
        （1）我们在将栈的应用时，讲到函数调用栈来保存临时变量，为什么函数调用"栈"来保存临时变量，
            用其他数据结构不行吗？
            
            答：因为函数调用的执行顺序符合后进者先出，先进者后出的特点。
                函数调用中经常嵌套，例子：A调用B，B又调用C，那么就需要先把C执行完，结果赋值给B中的临时变量，
                B的执行结果再赋值给A的临时变量，嵌套越深的函数越需要被先执行，这样刚好符合栈的特点，
                因此每次遇到函数调用，只需要压栈，最后依次从栈顶弹出依次执行即可，
                根据数据结构是特定应用场景的抽象的原则，我们优先考虑栈结构。
            
        （2） JVM 内存管理中有个 “堆栈” 的概念。栈内存用来存储局部变量和方法调用，
              堆内存用来存储 java 中的对象。那么 JVM 里面的“栈”和我们这里说的“栈”
              是不是一回事呢？如果不是，为啥都叫做“栈”呢？
            
            答：内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构。
            内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区，动态数据区又分为栈区和堆区。
            代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。
            静态数据区：存储全局变量、静态变量、常量，常量包括final修饰的常量和String常量。系统自动分配和回收。
            栈区：存储运行方法的形参、局部变量、返回值。由系统自动分配和回收。
            堆区：new一个对象的引用或地址存储在栈区，指向该对象存储在堆区中的真实数据。    
            
              
        
"""队列：队列在线程池等有限资源池中的应用"""                  
        
    1、如何理解队列：
    
        先进先出，就是最基本的队列
        队列两个最基本的操作：入队（enquenue()）,放一个数据到队列尾部；出队（dequeue()）,从队列头取出一个元素。
        和栈一样，队列也是一种操作受限的线性表数据结构。
        
        作为一种非常基础的数据结构，队列的应用非常广泛，特别是一些具有额外特性的队列：
        比如：循环队列、阻塞队列、并发队列。
        
    2、顺序队列和链式队列：
    
        队列和栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持队尾插入元素，在对头删除元素。
        用数组实现的队列叫顺序队列，用链表实现的队列叫链式队列。
        
        用数组实现队列，随着不停是入队和出队操作，head 和 tail 往后移动，当 tail　移动到最右边，即使
        数组还有空闲空间（数组头的位置，因为head 出队不断后移），也无法继续往队列中添加数据了，
        这时就要进行数据搬移，迁移到一个更大的数组。
           
    3、循环队列：
        
        在 tail == n 时，会有数据搬移操作，这样入队操作性能就会受到影响，那么有没有一种办法能够避免
        数据搬移呢？看看循环队列的思路
        
        循环队列，顾名思义，它长得像一个环，原本数组有头为尾，是一条直线，现在我们把首尾相连。
        
        假设 队列大小为 8（0,1,2,3,4,5,6,7） ，当前head = 4， tail = 7，当有一个新的元素 a 入队时，我们放入下标
        为 7 的位置，我们并不是把 tail 更新为8，而是将其在环中后移以为，将移动到下标为 0 的位置。
        当再有一个元素 b 入队时，将 b 放入下标为 0 的位置，然后tail 加 1 更新为 1.
        通过这种方法，成功避免了数据搬移操作。
        
        最关键的时，确定好队空和对满的判断条件。
            
            在用数组的非循环队列中，队满的判断条件是 tail == n(数组大小)，队空的判断条件是（head == tail）
        
            循环队列，队空的判断条件依然是 head == tail，队列满的条件是 （tail + 1）%n = head.
            当对满时，tail 指向的位置实现上是没有存储数据的，循环队列会浪费一个数组的存储空间。
            
            
    4、阻塞队列和并发队列：
        
        阻塞队列其实就是在队列的基础上增加了阻塞操作。简单来说，就是队列为空的时候，
        从队头取数据会被阻塞，因为没有数据可取，直到队列中插入新的数据才能返回。
        队满的时候，插入数据操作会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。
        我们使用阻塞队列可以实现一个“生成者--消费者模式”
        
        并发队列：又叫线程安全队列，最简单直接的方法是 enqueue(),dequeue()方法上加锁，
        但是锁粒度大并发度会比较低，同一时刻仅允许一个存或取操作。
        实际上，基于数组的循环队列，利用了 CAS 原子操作，可以实现高效并发。
        
    5、线程池没有空闲线程时，新的任务请求资源时，线程池该如何处理，各种处理策略如何实现
    
        （1）非阻塞处理方式，直接拒绝任务请求
        （2）阻塞处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。
            队列正是这样数据结构来存储排队请求。
            基于链表的实现方式，可以支持一个无限的排队的无界队列，但是可能会导致
            请求排队等待，请求处理的相应时间过长。
            基于数组的有界队列的大小有限，所以线程池中排队的请求超过队列大小时，
            接下来的请求会被拒绝。
            
        实际上，对于大部分的资源有限场景，当没有空闲资源时，基本上都可以用“队列”这种、
        数据结构来实现请求排队。
        
    6、课后思考：
    
        （1）除了线程池结构用到队列排队请求，还有那些会用到队列排队请求？
            分布式中的消息队列，也是一种队列结构
        
        （2）如何实现无锁并发队列？
            无锁队列主要是通过CAS、FAA这些原子操作，和Retry-Loop实现。
            对于Retry-Loop，我个人感觉其实和锁什么什么两样。
            是这种“锁”的粒度变小了，主要是“锁”HEAD和TAIL这两个关键资源。而不是整个数据结构。
        
        
"""10 递归：如何用三行代码找到最终推荐人"""           
    
    数据结构和算法两个最难理解的知识点：动态规划和递归
    1、如何理解“递归”
        
        一个非常标准的递归求解问题的分解过程，去的那个过程叫“递”，回来的过程叫“归”。
        基本上所有的递归问题都可以用递推公式来表示。
        
        生活中的电影院第几排的例子，我们用递推公式将它表示出来就是：
        
            f(n) = f(n - 1) + 1 其中 f(1) = 1
         
            f(n) 表示想知道自己在哪一排，f(n-1) 表示前面一排所在的排数，f(1)表示第一排知道自己在第一个排。
            有了这个递推公式，我们很轻松递归代码：
            
            int f(int n){
                if(n == 1) return 1;
                return f(n - 1) + 1
            } 
            
    2、递归需要满足的三个条件：
    
        （1）一个问题的解可以分解为几个子问题的解。
            
            何为子问题？子问题就是数据规模更小的问题。
            
        （2）这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样。
            
            例如上例中的，自己在那一排的思路和前面一排求解自己在哪一排的思路是一样的，
            就是 前排 + 1
            
        （3）存在递归终止条件。
            
            把问题分解为子问题，吧子问题再分解成子子问题，一层一层分解下去，不能存在无限循环
            这就需要终止条件，上例中的 f(1) = 1 就是终止条件  
                            
    3、如何编写递归代码：
        
        写递归代码的关键是写出递归公式，找到终止条件，剩下就是让递归公式转换成代码。
        
        假设有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问 走这 n 个台阶有多少中走法。
        如果有 7 个台阶，你可以 2，2，2，1 这样子走，也可以1， 2， 1， 1， 2 这样子走，那么
        如何用编程求得总共有多少中走法呢？
        
        仔细想想，实际上，可以根据第一步的走法把所有走法分成两类，第一类是第一步走 1 个台阶，
        另一类是第一步走了 2 个台阶。多以 n 个台阶的走法就是先走 1 阶后，n - 1 个台阶的走法
        加上先走 2 个阶后，n - 2 个台阶的走法。
        用公式表示就是：
                        f(n) = f(n - 1) + f(n - 2)
                        
        我们再看看终止条件，当有一个台阶时，我们不需要在继续递归，就只有一种走法，所以 f(1) = 1.
        还有f(2) = 2 作为一种终止条件，表示 2 个台阶，有两种走法，一步走完，或者分两步来走，
        所以，终止条件就是 f(1) = 1, f(2) = 2
        
        我们把递归终止条件和递归公式放在一起就是这样：
        
            f(1) = 1
            f(2) = 2
            f(n) = f(n-1) + f(n-2)
        
        有了这个公式，转成代码就简单多了，最终的代码公式是：
        
            int f(int n){
                if (n == 1) return 1;
                if (n == 2) return 2;
                return f(n - 1) + f(n - 2);
            }        
    
        写递归代码的关键是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式。
        然后再推敲最终条件，最终将递推公式和终止条件翻译成代码。
    
        如果递归调用只有一个分支，也就是说“一个问题只需要分解为一个子问题”我们很容易想清楚
        “递” 和 “归” 的每一个步骤，所以写起来和理解起来都不难。
        
        但是，当我们面对的是一个问题要分解成多个子问题的情况，递归代码就没有那么好理解。
        比如上面的例子。
    
    4、 怎样理解递归：    
        
        计算机擅长做重复的事情，所以递归正和它的胃口，而人脑喜欢平铺直叙的思维方式，当我们看到递归时，
        我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚
        计算机每一步都是怎样执行的，这样就很容易被绕进去。
        
        对于递归代码，这种试图想弄清楚整个“递” 和 “归” 过程的做法，实际上有很多误区，很多时候
        理解起来比较吃力，主要是自己给自己造成的理解障碍，那正确的思维方式应该怎样？
        
        如果一个问题 A 可以分解为若干个字问题 B、C、D, 你可以假设子问题 B、C、D 都已经解决，
        在此基础上思考如何解决问题 A. 而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可。
        不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节。
        
        因此，编写递归代码的关键是，只要需要递归，我们就把它抽象成一个递推公式，不用想层层调用关系，
        不用试图用人脑去分解递归的每个步骤。
        递归代码要警惕堆栈溢出，指定要递归退出条件，我们可以在代码中限制递归调用的最大深度方式来解决。
        递归调用一定深度之后，我们就不继续往下再递归了，直接返回保错。
        
            int depth = 0;
            int f(int n){
                ++depth;
                if (depth >1000) throw exception;
                
                if(n == 1)return 1;
                return f(n - 1) + 1
            }
        
        
    5、递归代码要警惕重复计算：
    
        使用递归时还会出现重复计算的问题，比如刚刚我们把递归过程分解一下，
        f(6) = f(5) + f(4)
        f(5) = f(4) + f(3)
        f(4) = f(3) + f(2)
        .....
        
        从分析，我们可以直观看到，想要计算 f(5), 需要先计算 f(4) 和 f(3),
        而计算 f(4) 还需要计算 f(3), 因此 f(3) 就被计算很多次，这就是重复计算的问题。
        为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经解决过的 f(k).
        当递归调用到 f(k) 时，先看下是否求解过，如果是，则直接从散列表中取值返回，不需要
        重复计算，这样就避免刚刚讲过的问题。
        
        public int f(int n){
            
            if(n == 1) return 1;
            if(n == 2) return 2;
            
            if(hasSolvedList.containsKey(n)){
                return hasSovledList.get(n)
            }
            
            int ret = f(n - 1) + f(n - 2);
            hasSovedList.put(n, ret)
            return ret
        }
        
        除了堆栈溢出，重复计算这两个问题，递归代码还有很多别的问题。
        比如时间效率。  
           
    6、怎样将递归代码改写为非递归代码？
        
        递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；
        而弊端是空间复杂度高，有栈溢出风险、存在重复计算、过多的函数调用会耗时比较多问题。
         
   
        比如上面的两个例子：
        
            int f(int n){
                int ret = 1;
                for(int i = 2; i <= n; ++i){
                    ret = ret + 1;
                }
                return ret;
            }
            
        第二个例子：
        
            int f(int n){
                if (n == 1) return 1;
                if (n == 2) return 2;
                
                int ret = 0;
                int pre = 2;
                int prepre = 1;
                
                for(int i = 3; i <= n; i++){
                    ret = pre + prepre;
                    prepre = pre;
                    pre = ret;
                }
                return ret;
            }    
            
        是不是所有的递归代码都可以改为 迭代循环的非递归代码？
        笼统的说，是的，因为递归本身就是借助栈来实现的，不过我们使用的栈是系统或虚拟机本身提供的。
                 
    7、如何实现“最终推荐人”
    
        long findRootReferrerID(long actorId){
            Long referrerId = select referrer_id from [table] where actor_id = actorId;
            if (referrerId == null) retrun actorId;
            return findRootReferrerId(referrerId)
        }        
        
        两个问题：   递归很深时，可能会有栈溢出的问题
                    如果数据库存在脏数据，我们还要处理由此出现的无限循环问题如：a-b-c-a
    8、内容小结：
    
        递归时一种高效、简洁的编码技巧，只有满足“三个条件”的问题都可以通过递归代码来解决。
        递归有很多弊端：如 堆栈溢出、重复计算、函数调用耗时、空间复杂度高等问题。
        
        不过递归也难写、难理解。写递归正确的姿势是写出递推公式，找出终止条件，然后再翻译成递归代码。


"""11 排序(上)：为什么插入排序比冒泡排序更受欢迎"""                       
     
    插入排序和冒泡排序的时间复杂度都是 O(n^2), 在实际的软件开发里，为什么我们更倾向于使用
    插入排序算法而不是冒泡排序算法呢？
    
    1、如何分析一个“排序算法”
        
        学习排序算法，我们除了学习它的算法原理、代码实现外，更重要的是要学会如何评价，分析一个排序算法。
        那么分析一个排序算法，要从那几个方面入手？
        
        （1）排序算法的执行效率
            
            a. 最好情况、最坏情况、平均情况时间复杂度, 为什么要区分三种时间复杂度？
                第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分
                第二，对于要排序的数据，有些接近有序，有些完全无序，对于不同的数据排序的执行时间肯定是有
                    影响的，我们要知道排序算法在不同数据下的性能表现。
                
            b. 时间复杂度的系数、常数、低阶。
                时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低级。
                我们排序 n 很小的时候，在对于同一阶时间复杂度排序算法性能对比的时候，我们要把系数、常数、低级考虑进来。
            
            c. 比较次数和交换（或移动）次数。
                
                基于比较的排序算法的执行过程，会涉及两种操作，一种是比较元素大小，另一种是元素交互或移动。
                所以，我们在分析排序算法的执行效率时，应该把比较次数和交换移动次数也考虑进去。
        
        （2）排序算法的内存消耗
        
                算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。
                不过，针对排序算法的空间复杂度，我们引入了一个新的概念，原地排序（Sorted in place）.
                原地排序就是指空间复杂度是 O(1) 的排序算法。
        
        （3）排序算法的稳定性
        
            稳定性是衡量排序算法的一个重要指标。
            稳定性：如果待排序的序列中存在的值相等，经过排序之后，相等元素之间原有的先后顺序不变。
            
            比如一组数据：2,9,3,4,8,3， 安照大小排序之后是2,3,3,4,8,9
            这组数据里有两个 3， 经过某种排序算法排序之后，如果两个 3 的前后顺序没有发生改变，
            那么，我们把这种排序算法叫做稳定的排序算法，否则就是不稳定排序算法。
            
            为什么要考察排序算法的稳定性呢？
                在真正的软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照某个 key 来排序。
                比如说，我们现在要给电商交易系统中的“订单”排序，订单有两个属性，一个是下单时间，一个是金额。
                如果现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序，对应相同金额的订单，我们
                希望从下单的早到晚排序，我们改怎么做？
                
                有一种办法：先按照金额进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照
                下单时间排序。这种思路理解起来不难，但是实现起来很复杂。
                
                借助稳定的排序算法，这个问题可以非常简洁的解决。思路如下：
                    先按照下单时间进行排序，排序完成之后，我们使用稳定算法，按照金额大小重新排序。
                    两次排序之后，我们的订单数据是按金额重按大小排序，金额相同的订单按下单时间从早到晚排序。
                    
                    稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变，金额相同的订单仍然
                    保持下单时间从早到晚。
                    
    2、冒泡排序（bubble sort）：
    
        def bubble_sort(alist):
            for i in range(len(alist)-1, 0, -1):
                for j in range(i):
                    if alist[j] > alist[j + 1]:
                        alist[j], alist[j + 1]  = alist[j + 1] , alist[j]    
     
        第一：冒泡排序是原地排序算法吗？
            冒泡排序的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，
            所以它的空间复杂度为 O(1),是一个原地排序算法。
            
        第二：冒泡排序是稳定的排序算法吗？
            在冒泡排序中，只有交换才可以改变两个元素的前后顺序，为了保证冒泡排序算法的稳定性，
            当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变
            顺序，所以冒泡排序是稳定的排序算法。
            
        第三：冒泡排序的时间复杂度是多少？
            
            最好情况，有序数据已经有序了，我们只需要进行一次冒泡操作，时间复杂度是O(n)
            最坏情况，要排序的数列刚好是倒序的，时间复杂度是 O(n^2)
            平均情况，平均时间复杂度是加权平均期望时间复杂度。
            
            对于包含 n 个数据的数组，这 n 个数据就有 n! 种排序方式。不同的排序方式，
            冒泡排序执行的时间肯定不同。我们通过“有序度” 和 “逆序度”这两个概念来进行分析。
            
            有序度：是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：
                
                有序元素对： a[i] <= a[j], 如果 i < j.
                
                [2, 4, 3, 1, 5, 6] 这组数据的有序度为 11， 因为有序元素对为 11，分别是：
                (2,4),(2,3),(2,5),(2,6),(4,5),(4,6),(3,5),(3,6),(5,6)。    
            
            同理，对于一个倒序排序的数组，比如6,5,4,3,2,1 有序度是 0；
            对于一个完全有序的数组，比如 1,2,3,4,5,6 有序度就是 n*(n-1)/2,
            我们把这种完全有序的数组的有序度叫作满有序度。
            
            逆有序度刚好相反，数据表达式就是：
                
                逆序元素对：a[i] > a[j], 如果 i < j。
                逆序度 = 满有序度 - 有序度。
                 
            我们排序的过程就是一种增有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成。
            
            冒泡例子：
                要排序的数组初始状态是 [4,5,6,3,2,1],其中有序对是(4,5)(4,6)(5,6),所以有序度是3。
                n = 5,排列完成之后最终满有序度为 n*(n - 1)/2 = 15.
                冒泡排序包含两个原子操作，比较和交换，每交换一次，有序度就加 1，不管算法怎么改进，
                交换的次数总是确定的，即为逆序度。15-3 = 12次交换操作。
                
                对于 包含 n 个素组进行冒泡排序，平均交换次数是多少呢？
                最坏情况是，初始状态有序度为 0，需要进行 n*(n -1 )次交换。
                最好情况是，初始状态有序度为 n(n - 1),不需要进行交换。
                我们可以取中间值 n*(n-1)/4,来表示初始有序度既不是很高
                也不是很低的平均情况。
                
           所以平均复杂度是 O(n^2)
           
    3、插入排序（Insertion Sort）
        
        一个有序数组，我们往里面添加一个新的数据，如何继续保持数据有序呢？
        很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。
        这是一个动态排序的过程，即动态的往有序集合中添加数据，我们可以通过这种方法
        保持集合中的数据一直有序。
        而对于一组静态数据，我们也可以借鉴上面插入的方法，来进行排序，于是就有插入排序算法。
        
        那插入排序具体是如何借助上面的思想来实现排序呢？
        
            首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素。
            就是数组的第一个元素，插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适
            的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序的元素为空，算法结束。
            
            插入排序也包含两个操作，一种是元素比较。一种是元素移动。
            当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，
            找到合适的插入位置。找到插入位置之后，我们还需要将插入点之后的元素顺序往后移动一位。
            这样才能腾出位置给元素 a 插入。
            
            对与不同的插入点方法（从头到尾，从尾到头）元素的比较次数是有区别的，但对于一个给定的初始化
            序列，移动操作的次数总是固定的，就等于逆序度。
            
            def insert_sort(alist):
                for i in range(1, len(alist)):
                    for j in reange(i, 0, -1):
                        if alist[j] < alist[j - 1]:
                            alist[j], alist[j - 1] = alist[j-1], alist[j]

        第一： 插入排序是原地排序算法吗？
            空间复杂度是O(1),是原地排序算法。
            
        第二：排序算法是稳定排序算法吗？
            在插入元素时，对于值相同的元素，我们可以选择在后面出现的元素插入到前面出现元素的后面。
            这样就可以保证原有前后顺序不变，所有插入排序是稳定排序算法。
            
        第三：插入排序的时间复杂度是多少？
            最好时间复杂度 O(n)
            最坏时间复杂度 O(n^2)
            平均时间复杂度 O(n^2)
            
    4、选择排序（Selection sort）:
    
        选择排序算法是实现思路有点类似插入排序，也分已排序区间和未排序区间。
        但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。
        
            select_sort(alist):
                for i in range(len(alist) - 1):
                    min_index = i
                    for j in range(i+1, len(alist)):
                        if alist[min_index] > alist[j]:
                            min_index = j
                    if min_index != i:
                        alist[min_index], alist[i] = alist[i], alist[min_index]        
                 
        空间复杂度 O(1)
        最好时间复杂度、最坏时间复杂度、平均时间复杂度都是 O(n^2)
        
        选择排序算法是稳定的排序算法吗？
            答案是否定的，选择排序是一种不稳定的排序算法，选择排序每次都要找到剩余末排序中最小值，
            并和前面的元素交换位置，这样破坏了稳定性。
            比如：[5,8,5,2,9] 这样的一组数据，第一次找到最小元素 2，与第一个 5 交换位置，
            那么 第一个 5 和 中间的 5 顺序就变了，所以不稳定。
             
    5、小结：
    
        特定的算法依赖特定的数据结构，三种时间复杂度都是 O(n^2), 适合小规模的排序。
        冒泡和选择排序，可能纯粹停留在理论层名，学习是为了开拓思维，实际中用到的并不多。
        插入排序还是挺有用的。
        
        

"""12 排序（下）：如何用快排思想在 O(n) 内查找第 K 大元素"""

    归并排序的原理:
        
        归并排序使用的是分治的思想，分治是一种解决问题的处理思想，
        递归是一种编程技巧，递归来实现分治。
        
        def merge(left, right):
            '''合并操作，将两个有序数组left[]和right[]合并成一个大的有序数组'''
            #left与right的下标指针
            l, r = 0, 0
            result = []
            while l<len(left) and r<len(right):
                if left[l] < right[r]:
                    result.append(left[l])
                    l += 1
                else:
                    result.append(right[r])
                    r += 1
            result += left[l:]
            result += right[r:]
            return result
        
        def merge_sort(alist):
            if len(alist) <= 1:
                return alist
            # 二分分解
            num = len(alist)//2
            left = merge_sort(alist[:num])
            right = merge_sort(alist[num:])
            # 合并
            return merge(left,right)   
    
        if __name__ == "__main__":

            alist = [54,26,93,17,77,31,44,55,20]
            sorted_alist = merge_sort(alist)
            print(sorted_alist)

    2、归并排序的性能分析：
    
        第一，归并排序是稳定的排序算吗？
            归并排序是一个稳定的排序算法
            
        第二：归并排序的时间复杂度：
        
            O(nlongn)
                   
        第三：归并排序的空间复杂度是多少：
            
            O(n)
            
    
    3、快速排序的原理：
    
        快排利用的也是分治思想。
        
        def quick_sort(alist, start, end):
            """快速排序"""
        
            # 递归的退出条件
            if start >= end:
                return
        
            # 设定起始元素为要寻找位置的基准元素
            mid = alist[start]
        
            # low为序列左边的由左向右移动的游标
            low = start
        
            # high为序列右边的由右向左移动的游标
            high = end
        
            while low < high:
                # 如果low与high未重合，high指向的元素不比基准元素小，则high向左移动
                while low < high and alist[high] >= mid:
                    high -= 1
                # 将high指向的元素放到low的位置上
                alist[low] = alist[high]
        
                # 如果low与high未重合，low指向的元素比基准元素小，则low向右移动
                while low < high and alist[low] < mid:
                    low += 1
                # 将low指向的元素放到high的位置上
                alist[high] = alist[low]
        
            # 退出循环后，low与high重合，此时所指位置为基准元素的正确位置
            # 将基准元素放到该位置
            alist[low] = mid
        
            # 对基准元素左边的子序列进行快速排序
            quick_sort(alist, start, low-1)
        
            # 对基准元素右边的子序列进行快速排序
            quick_sort(alist, low+1, end)
        
        
        if __name__ == "__main__":
        
            alist = [54, 26, 93, 17, 77, 31, 44, 55, 20]
            print(alist)
            quick_sort(alist, 0, len(alist) - 1)
            print(alist)
    
    4、快排性能分析：
    
        快排是一种原地不稳定的排序算法，
        时间复杂度 O(nlogn)
                       
    5、解答开篇：
    
        利用快排的核心思想是分治和分区，我们可以分区思想，来解答开篇的问题：
        O(n) 时间复杂度内求无序数组的第 K 大元素。比如:[4, 2, 12, 3, 5]
        这样一组数据，第 3 大元素就是。
         
        我们选择数组区间 A[0...n-1]的最后一个元素 A[n-1] 作为 pivot, 对数组
        A[0...n-1]原地分区，这样数组就分成了三部分，A[0...p-1]、A[p]、A[p+1...n-1]。
        
        如果 p+1=k, 那么 A[p] 就是要求解的元素，如果 K>p+1,说明第 K 大元素出现在
        A[p+1...n-1]区间，我们再按照上面的思路递归地在 A[p+1...n-1]这个区间内查找。
        同理，如果 K<p+1,那我们在 A[0...p-1]区间查找。



"""13| 线性排序：如何根据年龄给100万用户数据排序"""        

    桶排序、计数排序、基数排序的时间复杂度是O(n), 时间复杂度是线性的，
    所以，我们把这类排序算法叫作线性排序（Linear sort）。
    这三种算法都不涉及到元素之间的比较操作。
    
    1、桶排序(Bucket sort)
    
        核心思想是将排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。
        桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。
        
        桶排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。
        这样每个桶内的数据排序完之后，桶与桶之间的数据不需要再进行排序。
        
        桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中。
        
    2、计数排序(Counting sort)
        
        计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，
        比如最大值是 k，我们可以把数据划分成 k 个桶。每个桶里面的数值都是相同的，省掉了
        桶内排序的时间。
        
        计数排序的算法思想就是这么简单，更桶排序非常类似，只是桶的大小粒度不一样。不过，
        为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？
        
        想弄明白这个问题，我们就要来看计数排序算法的实现方法。
        假设只有 8 个考生，分数在 0 到 5 分之间。这 8 个考生的成绩我们放在一个数组 A[8]中
        他们分别是：2，5，3，0，2，3，0，3。
        
        考生的成绩从 0 到 5 分，我们使用大小为 6 的数组 C[6] 表示桶，其中下标对应分数，C[6]
        内存储的并不是考生，而是对应的考生个数。我们只需要遍历一遍考生分数，就可以得到 C[6]的值。
            
            C[6]  2  0  2  3  0  1  内容--个数 
                  0  1  2  3  4  5  下标--分数
                  
        从图中可以看出，分数为 3 分的考生有 3 个，小于 3 分的考生有 4 个，所以，成绩为 3 分的
        考生在排序之后的有序数组 R[8] 中，会保存下标 4，5，6 的位置。
        
            R[8]  0  0  2  2  3  3  3  5      
                  0  1  2  3  4  5  6  7
        
        那么我们如何快速计算出，每个分数的考生在有序数据中的存储位置呢？
        这个处理方法非常巧妙，很不容易想到。
        
        思路是这样的：我们对 C[6] 数据顺序求和，C[6]存储的数据就变成了下面这样子。
        C[k]里存储小于等于分数 K 的考生个数。
        
            C[6] 2  2  4  7  7  8
                 0  1  2  3  4  5
                 
        我们从后到前依次扫描数组 A[2，5，3，0，2，3，0，3], 比如，当扫描到 3 时
        我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内
        分数小于等于3 的考生有7个，也就是说 3 是数组 R 中的第 7 个元素(也就是数组R
        中下标为 6 的位置)，当把 3 从A数组中取出放入到数组R中后，小于等于 3 的元素
        就只剩下了 6 个了，所以相应的 C[3] 要减 1， 变成 6。
        
        依次类推，当我们扫描到第 2 个分数为 3 的考生的时候，就会把它放入数组R中第6个元素
        的位置(也就是下标为 5 的位置)。当我们扫描完整个数组 A 后，数组 R 内的数据就是按照
        分数从小到大有序排序的了。
        
        这种利用另外一个数组来计数的实现排序算法的方式叫做计数排序，
        计数排序只能用在数据范围不大的场景中，如果数据范围 k 要比排序
        的数据 n 大很多，就不适合计算排序了，如果要排序的数据是其他类型的
        要将其在不改变相对大小的情况下，转换为非负整数。
        
        # 计数排序，a 是数组，n 是数组大小，
        # 假设数组中存储都是非负整数。
        
        def countingSort(a, n):
        
            if n <= 1:
                return
        
            # 查找数组中数据的范围
            max = a[0]
            for i in range(1, n):
                if(max < a[i]):
                    max = a[i]
        
            # 申请一个计数数组 c, 下标大小为[0, max]
            c = [0]*(max + 1)
            #print(c)
            #print('---------------------')
        
            # 计算每个元素的个数，放入 c 中
            for i in range(n):
                c[a[i]] += 1
            #print(c)
            #print('---------------------')
        
        
            # 依次累加
            for i in range(1, max + 1):
                c[i] = c[i - 1] + c[i]
            #print(c)
            #print('---------------------')
        
            # 临时数组 r, 存储排序之后的结果
            r = [None]*n
        
            for i in range(n-1,-1,-1):
                index = c[a[i]] - 1
                r[index] = a[i]
                c[a[i]] -= 1
            #print(r)
        
            # 将结果拷贝给 a 数组
            for i  in range(n):
                a[i]= r[i]
        
        if __name__ == "__main__":
        
            nums = [2, 5, 3, 0, 2, 3, 0, 3]
            print(nums)
            countingSort(nums, len(nums))
            print(nums)
 
        
    3、基数排序：
    
        假设我们用 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，
        你有什么比较快的排序方法呢？
        
        刚刚的问题里有这样的规律：假设要比较两个手机号 a, b 的大小，如果在
        前面几位中，a 手机号码已经比 b 手机号码大， 那后面位就不用看了。
        
        借助稳定排序算法，先按照最后一位来排序手机号码，然后，再按照倒数第二位
        重新排序，依次类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码都有序了。
        
        注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的，
        因为如果非稳定排序算法，那最后一次排序只会考虑最高的大小顺序，完全不管其他
        位的大小关系，那么低位排序就完全没有意义了。
        
        基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间
        有递进关系，如果 a 数据高位比 b 数据大，那剩下的低位就不用比较了，除此之外
        每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基础排序的时间
        复杂度无法做到O(n)
        
    4、小结：
    
        桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分为
        不同的桶来实现排序。
        基数排序要求可以划分成高低位，位之间有递进关系。而且每一位的数值范围不能太大
        因为基数排序需要借助桶排序或者计数排序来完成每个位的排序工作。
        
        
        
"""14| 排序优化：如何实现一个通用、高性能的排序函数"""        
    
    快排是一种比较好的排序方法。
    如何优化快速排序：
        
        选择合适的分区点，避免递归太深：
            分区点的取法：
                三数取中法
                随机法
    
    没有标准答案，思考比标准答案更重要。  
                
                
"""15|二分查找(上): 如何用最省内存的方式实现快速查找功能"""                
        
    二分法针对的是一个有序的数据集合，查找思想有点类似   
    分治思想，
    每次都通过分区间的中间元素对比，将带查找的区间缩小为之前的一半，
    直到找到要查找的元素，或者区间被缩小为 0。
    
    1、二分法查找的时间复杂度：
        
        n/2^k = 1 求得 k = log2n, 时间复杂度是 O(logn)。
        
    
    2、二分查找的递归与非递归实现：
    
        def binary(nums, number):
            low = 0
            high = len(nums) - 1
            while low <= high:
                mid = (low + high) // 2
                if nums[mid] == number:
                    return True
                elif nums[mid] > number:
                    high = mid - 1
                else:
                    low = mid + 1
            return False
        
        if __name__ == "__main__":
        
            nums = [1, 4, 5, 8, 9, 12, 23, 33, 44, 56]
            print(binary(nums, 12))
        
        注意：low, high, mid 都是指数组下标，其中low,high表示当前查找的范围，
        初始 low = 0, high = len(nums) - 1, mid 表示[low, high]中间值。
        三个出错点：
        
            a、循环退出条件：
                注意是 low<=high, 而不是 low< high
            
            b、mid 取值：
            
                mid = (low+high)/2 这种写法有问题，因为如果low 和 high 比较大的话
                两者之和可能会溢出。改进方法是 low + (high-low)/2。
                更进一步可以将除以2转化成为运算 low + ((high - low) >> 1)
            
            c、low 和 high 的更新：
                
                low = mid + 1, high = mid - 1, 
                如果直接写成 low = mid 或者 high = mid，就可能发生死循环。
                
            
        递归实现：
        
            def binary(nums, low, high, val):
                if low > high:
                    return False
                mid = low + ((high-low) >> 1)
                if nums[mid] == val:
                    return True
            
                elif nums[mid] < val:
                    return binary(nums, mid+1, high, val)
                else:
                    return binary(nums, low, mid-1, val)
            
            
            if __name__ == "__main__":
            
                nums = [1, 4, 5, 8, 9, 12, 23, 33, 44, 56]
                print(binary(nums, 0, len(nums)- 1, 22))    
        
        
    
    
    
    3、二分法应用场景的局限性：
    
        首先，二分查找依赖的是顺序表结构，简单的说就是数组。
        二分查找是否依赖其他数据结构？比如链表，答案是不可以的，
        主要原因是二分查找算法需要按照下标随机访问元素。
        
        其次，二分查找针对的是有序数据。
        所以，二分查找只能用在插入、删除不频繁，一次排序多次查找的场景中。
        
        再次，数据量太小不适合二分查找。
        
        最后，数据量太大也不适合二分查找。
        因为，二分查找底层需要依赖数组这种数据结构，而数组为了支持随机性访问的特性，
        要求内存空间连续，对内存的要求比较苛刻。
       
        
        
"""16| 二分查找(下): 如何快速定位 IP 对应的省份地址"""        
    
    1、四种常见的二分查找变形问题：
    
        a. 查找第一个值等于给定值的元素
        
        b. 查找最后一个值等于给定值的元素
        
        c. 查找第一个大于等于给定值的元素
        
        d. 查找最后一个小于等于给定值的元素
        
    2、变体一：查找第一个值等于给定值的元素
        
        有序集合中存在重复的数据，我们希望找到第一个值等于给定值的数据，
        比如这样一个有序数组 [1, 3, 4, 5, 6, 8, 8, 8, 11, 18]，
        其中 a[5], a[6], a[7] 的值都等于 8，是重复的数据。我们希望
        查找到第一个等于 8 的数据，也就是下标是 5 的元素。
        
           
        def binary_search(nums, val):
            low = 0
            high = len(nums) - 1
            while low <= high:
                mid = low + ((high - low) >> 1)
                if nums[mid] > val:
                    high = mid - 1
                elif nums[mid] < val:
                    low = mid + 1
                else:
                    if (mid == 0) or (nums[mid - 1] != val):
                        return mid
                    else:
                        high = mid - 1
            return -1
        if __name__ == "__main__":
            print(binary_search([1, 3, 4, 5, 6, 8, 8, 8, 11, 18], 8))
        
        nums[mid] 要跟查找的 val 的大小关系有三种情况，大于，小于，等于。
        当 nums[mid] = val 时：如果我们查找的是任意一个值等于给定值的元素，
        当 nums[mid] 等于要查找的值时， nums[mid] 就是我们要查找的元素。但是
        如果我们求解的是第一个值等于给定的元素，当 nums[mid] = val 时，我们
        就需求判断一下，这个 nums[mid] 是不是第一个值等于给定的元素。
        如果 mid = 0, 那么这个元素已经是数组的第一个元素，那肯定是我们要找到
        如果 mid 不等于 0，但 nums[mid-1]不等于 val, 那么也说明 nums[mid] 就是
        我们要找的第一个值等于给定值的元素。

        如果检查发现 nums[mid - 1] = val, 说明mid 不是我们要的元素，
        更新 high = mid - 1, 因为要找的元素肯定出现在[low, mid - 1]之间。    
        
        
    3、 变体二: 查找最后一个值等于给定值的元素
        
        def binary_search(nums, val):

            low = 0
            high = len(nums) - 1
        
            while low <= high:
        
                mid = low + ((high - low) >> 1)
        
                if nums[mid] > val:
                    high = mid - 1
                elif nums[mid] < val:
                    low = mid + 1
                else:
                    if (mid == len(nums) - 1) or (nums[mid + 1] != val):
                        return mid
                    else:
                        low = mid + 1
        
            return -1
        
        if __name__ == "__main__":
        
            print(binary_search([1, 3, 4, 5, 6, 8, 8, 8, 11, 18], 8))
    
                
        我们稍微改变一下代码判断 nums[mid] = val 时，mid 是不是最后一个元素，
        如果不是最后一个元素，那么 low = mid + 1
        
        
    4、变体三: 查找第一个大于等于给定值的元素：
        
        在有序数组中，查找第一个大于等于给定值的元素，比如，数组中存储的这样一个序列
        [3, 4, 6, 7, 10] 如果查找第一个大于等于 5 的元素，那就是 6。
        
        实际上，实现的思路更前面的两种变形问题的实现思路类似：
        
        def binary_search(nums, val):

            low = 0
            high = len(nums) - 1
        
            while low <= high:
        
                mid = low + ((high - low) >> 1)
        
                if nums[mid] >= val:
                    if mid == 0 or nums[mid - 1] < val:
                        return mid
                    else:
                        high = mid - 1
                else:
                    low = mid + 1
        
            return -1
        
        if __name__ == "__main__":
        
            print(binary_search([3, 4, 6, 7, 10], 5))
        
        如果 a[mid - 1] 也是大于等于要查找的值 val, 说明要查找的元素在[low, mid-1]之间，
        所以，我们将 high = mid - 1。
        
    5、 变体四：查找最后一个小于等于给定值的元素：
    
        数组中存储 [3, 5, 6, 8, 9, 10], 最后一个小于等于 7 的元素就是 6，
        
        def binary_search(nums, val):
            low = 0
            high = len(nums) - 1
            while low <= high:
                mid = low + ((high - low) >> 1)
                if nums[mid] > val:
                    high = mid - 1
                else:
                    if mid == len(nums) - 1 or nums[mid + 1] > val:
                        return mid
                    else:
                        low = mid + 1
            return -1
        
        if __name__ == "__main__":
        
            print(binary_search([3, 4, 5, 6, 8, 9, 10], 7)) 
                             
    6、小结：
    
        二分法查找更适合使用在“近似”查找问题，在这类问题上，二分查找
        的优势更加明显。
        容易出错的细节：终止条件、区间上下界更新方法、返回值的选择。
        
                
                
"""17|跳表: 为什么 Redis 一定要用跳表来实现有序集合"""     

    上两节将的二分法查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。
    如果存储在链表中，就真的没有办法用二分法吗？
    实际上对链表稍加改造，就可以支持类似“二分法”的查找算法，我们把这种改造的
    数据结构叫作“跳表”。     
    
    跳表是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作
    写起来也不复杂，甚至可以代替红黑树。
        
    redis 中的有序集合就是用跳表来实现的。    
        
                
    1、如何理解“跳表”:
        
        对于一个单链表来讲，即便链表中存储的数据时有序的，如果我们想要查找                        
        某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是O(n)。
        
        怎样才能提高查询效率呢？
        如果想图中那样，对链表建立一级 “索引”，查起来是不是就会更快一些呢？
        每两个结点提取一个结点到上一级，我们把抽出来的那一级叫作索引或索引层。
        图中的 down 表示down 指针，指向下一级结点。
        
        1 级索引1 -----> 4 ------> 7 ------> 9 ---------> 13 ---- --> 17
                down 
        
        原始链表 1 -> 3 -> 4 -> 5 -> 7 -> 8 -> 9 -> 10 - > 13 -> 16 -> 17 -> 18         
        
        如果我们要查找某个结点，比如 16，我们可以先在索引层遍历，当遍历到索引层中值
        为 13 的结点时，我们发现下一个结点时 17，要查找的值在这两个结点中间，我们
        通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历，这个时候，只需要
        再遍历 2 个结点，就可以找到等于 16 的这个结点了。这样，原来如果查找 16需要
        遍历 10 个结点，现在只需要 7 个结点。
        
        从这个例子可知，加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是
        说查找效率提高了。
        
        跟上面的类似，我们第一级索引的基础上，每两个结点再抽出一个结点到第二级索引。
        现在要遍历 16，只需要遍历 6 个结点了，需要遍历的结点数又减少了。
        
        
        2 级索引1 ---------------> 7 -------------------> 13  
        
        1 级索引1 -----> 4 ------> 7 ------> 9 ---------> 13 ---- --> 17
                down 
        
        原始链表 1 -> 3 -> 4 -> 5 -> 7 -> 8 -> 9 -> 10 - > 13 -> 16 -> 17 -> 18  
        
        当数量很大的时候，比如 64 个结点，建立 5 级索引。
        
        这种链表加多级索引的结构，就是跳表。
        
    2、跳表查询到底有多快：
    
        每两个结点会抽出一个结点作为上一级索引的结点，那么第一级索引的结点大约是n/2
        第二级索引的结点个数大约就是 n/4, 第三级是 n/8,类似第 K 级 n /2^k
        
        假设索引有 h 级，最高级的索引有 2 个结点，通过上面的公式，我们可以得到
        n/2^h = 2, 从而得到 h = log2n - 1。如果包含原始链表这一层，这个跳表的高度
        就是log2n, 我们在跳表中查找某个数据的时候，如果每一层都要遍历 m 个结点，
        那么跳表中查询一个数据的时间复杂度就是 O(m*logn)。
        
        那么 m 的值是多少呢，我们每一层索引最多只需要遍历 3 个结点。
        假设我们要查找的数据是 x, 在第 K 级索引中，我们遍历到 y 结点之后，
        发现 x 大于 y 结点但是小于后面的 z 结点 ,所以从 y 结点 down 指针，
        从第 k 级索引下降到第 k - 1 级索引，在第 k - 1 级索引中， y 和 z 之间，
        只有 3 个结点(包含 y 和 z) ,所以，我们在 k - 1 级索引中最多只需要遍历 3 个结点
        一次类推，每一级最多只需要遍历 3 个结点。
        得到 m = 3,所以在跳表中的查找任意数据的时间复杂度就是 O(logn)。
        这个查询时间复杂度和二分查找一样。
        
    3、跳表是不是很浪费内存空间：
    
        跳表的空间复杂度并不难分析，假设原始链表大小为 n, 每升一级就减少一半，直到剩下2个结点
        如果我们把每一层的索引结点数写出来，就是一个等比数列：
            n/2、n/4、n/8、...、8、4、3
            
        几个索引的总和就是 n/2+ n/4 + m/8 +...+ 8 + 4 + 3 = n-2, 索引跳表的空间复杂度是 O(n)。
        也就是说，如果包含 n 个结点的单链表构造成一个跳表，我们需要再用接近 n 个结点的存储空间。
        能不能更省空间呢？
        
        上面是我们抽取 2 个结点构造上一级索引，如果每个三个结点，每5个结点抽取一个结点构造上一级索引。
        例如：3 个结点抽取一个
            n/3、n/9、n/27、...、9、3、1 = n/2。
        尽管空间复杂度还是 O(n)，但比上面两个结点减少了一半存储空间。
        
    4、高效动态插入和删除：
        
        跳表支持动态插入、删除操作，而且操作的时间复杂度也是 O(logn)。
        
    5、跳表索引动态更新：
    
        当我们不停的往跳表中插入数据时，如果我们不更新索引，就有可能出现某两个索引结点
        之间数据非常多的情况，极端情况下跳表还会退化成单链表。
        
        作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡。
        也就说，如果链表中结点多了，索引结点就要相应的增加一些，避免复杂度退化，
        已经查找、插入、删除操作性能下降。
        
        当我们往跳表中插入数据的时候，我们可选择同时将这个数据插入到部分索引层中
        如果选择加入那些索引层呢? 我们通过一个索引函数，来决定将这个结点插入到
        那几级索引中，比如随机函数生成值为 k, 我们将这个结点添加到第一级到第k级
        这 k 级索引中。
        
        代码见 Redis 中关于有序集合的跳表实现。
        
        跳表没有现成的实现，如果想使用跳表，要自己去实现。
        
    6、小结：
    
        跳表的时间复杂是 O(logn), 空间复杂度 O(n)。
        
        
        
"""18| 散列表(上)：Word 文档中的单词拼写检查功能是如何实现的"""        


    1、散列思想：
    
        散列表的英文叫“Hash Table”, 我们平时也叫 “哈希表”或者“Hash 表”。
        散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实
        就是数组的一种扩展，由数组演化而来，可以说，如果没有数组，就没有散列表。
        
        假设我们有 89 名参加学校运动会，为了方面记录成绩，每个选手的编号用6位
        数字表示。比如 051167. 05表示年级，11表示班级，最后两位时编号。
        尽管我们不能直接把编号作为数组下标，但我们可以截取参赛编号的后两位
        作为数组下标，来存取选手信息数据，当通过参数编号查询选手信息的时候，
        我们用同样的办法，去参赛编号的后两位，作为数组下标，来读取数组中的数据。
        
        这就是典型的散列思想，其中，参赛选手的编号我们叫做 “键(key)”或者关键字。
        我们用它来标识一个选手。我们把参数编号转化为数组下标的映射方法就叫散列函数
        (或 Hash 函数，哈希函数)，而散列函数计算得到的值就叫做散列值(或 Hash值 哈希值)。
        
        散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。
        我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的
        位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转换数组下标，
        从对应数组下标的位置取数据。
        
    2、散列函数：
    
        散列函数，顾名思义，它是一个函数，我们可以把它定义成 hash(key)，其中 key 
        表示元素的键值，hase(key) 的值表示经过散列函数计算得到的散列值。
        
        散列函数设计的3点基本要求:
            
            a. 散列函数计算得到的散列值是一个非负整数。
            b. 如果 key1 = key2, 那么 hash(key1) == hash(key2)。
            c. 如果 key1 != key2, 那么 hash(key1) != hash(key2)
        
        第一点是因为，数组的下标从 0 开始的。
        第三点看起来合情合理，但是在真实的情况下，要想找到一个不同的 key 对应
        的散列值都不一样的散列函数，几乎是不可能的，即便像业界著名的 MD5、SHA、CRC
        等哈希算法，也无法避免这种散列冲突。而且，数组的存储空间有限，也会加大散列
        冲突的概率。
        
    3、散列冲突：
    
        常用散列冲突解决方法有两类，开放寻址法(open addressing) 和 链表法(chaining)。
        
        a. 开放寻址法：
            开发寻址的核心思想就是，如果出现了散列冲突，我们就从新探测一个空闲位置，
            将其插入。那如何重新探测一个空闲位置呢? 
            一个比较简单的探测方法：线性探测 (Linear Probing).
            当我们往散列表中插入数据时，如果某个数据经过散列函数散列后，存储位置已经
            被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到位置。
            
            线性探测法其实存在很大问题，当散列表中插入的数据越来越多时，散列冲突发生
            的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。
            
            对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法
            二次探测(Quadratic probing) 和 双重散列(Double hashing)。
            
            所谓二次探测，更线性探测很像，跟线性探测很像，线性探测每次探测的步长是 1，
            那它探测的下标序列就是 hash(key)+ 0, hash(key)+1, hash(key)+2....
            而二次探测的步长就变成原来的二次方，也就是hash(key)+ 0, hash(key)+1^2, hash(key)+2^2....
            
            所谓双重散列，意思就是我们不是使用一个散列函数，我们使用一组散列函数 hash1(key),
            hash2(key),hash3(key), ... 我们先用第一个散列函数，如果计算的存储位置被占用，
            再用第二个散列函数，依次类推，直到找到空闲位置。
            
            不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。
            为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中的一定比例
            的空闲槽位，我们用装载因子(load factor)来表示空位的多少。
            
                散列表装载因子 = 填入表中的元素个数/ 散列表的长度
            
            装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。
            
        b、链表法：
        
            链表法是一种更常用的散列冲突解决办法。
                
        c、问题：当散列冲突，表中存储了多个相同散列值时，查询数据怎么确定查询到的是我
            想要的那个呢？
            答：全量对比，因为散列表中存储的不仅仅是哈希值，还有全量的数据信息。
            hash 值并不是一个 hash 函数的值，更好的表述应该是声明一个 count 字段。
   
                    
                    
"""19| 散列表(中): 如何打造一个工业级水平的散列表"""

    散列表中的查询效率并不能笼统的说成是 O(1), 它跟散列函数、装载因子、散列冲突等都有关系。
    如果散列函数设计的不好，或装载因子过高，都可能导致散列冲突发生的概率过高，都可能导致
    散列冲突的概率升高，查询效率下降。
    
    1、散列攻击：
    
        在极端情况下，有些恶意攻击者，通过精心构造数据，使得数据经过散列函数后，都散列到
        同一个槽里。如果使用基于链表的冲突解决方法，这个时候散列表就会退化为链表，查询
        复杂度从O(1)急剧退化为O(n)。
        这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，
        从而达到拒绝服务(DoS)的目的。这也就是散列表碰撞攻击的基本原理。
        
    2、如何设计散列函数:
    
        散列函数设计的好坏，决定了冲突散列表冲突的概率大小，也直接决定了散列表的性能。
        那什么才能是好的散列函数呢？
        
        首先，散列函数的设计不能太复杂。过于复杂的散列函数，势必消耗很多计算时间，
        也间接的影响到散列表的性能。
        其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突。
        散列到每个槽中的数据也会计较平均，不会出现某个槽内数据特别多的情况。
        
        实际工作中，我们还需要综合考虑各种因素，这些因素有关键字的长度、特点、分布、
        还有散列表的大小等。
        散列函数各式各样。比如直接寻址法、平方取中法、折叠法、随机数法等等。   
        
    3、装载因子过大了怎么办:
        
        装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率越大。
        不仅插入数据过程要多次寻找或者拉很长的链，查找的过程也会因此变的很慢。
        
        对于没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等
        设计出完美，极少冲突的散列函数，因为毕竟数据是已知的。
        
        对于动态散列表来说，数据集合是频繁变动的，我们事先无法预估将要加入的数据个数，
        所以我们也无法事先申请一个足够大的散列表，随着数据的加入，装载因子就会变大，
        当装载因子大到一定的程度后，散列冲突就会变得不可接受。这个时候，我们该如何处理。
        
        针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表
        将数据移到新的散列表中。假设每次扩容都申请一个是原来散列表的两倍大，那么新
        散列表的装载因子是原来的一半大。
        
        针对数据的扩容，数据搬迁操作比较简单，但是，针对散列表的扩容，数据搬迁操作
        要复杂很多，因为散列表的大小变了，数据的存储位置也变了，所以，我们需要通过
        散列函数重新计算每个数据的存储位置。
        
        散列表的扩容依据装载因子，装载因子的选择要适当，太大导致冲突过多，太小，就会导致内存浪费。
        
        
    4、如何避免低效的扩容：
    
        为了避免扩容的时候，插入数据会变得很慢，甚至无法接受。
        这个时候“一次性”扩容机制就不合适了，为了解决一次性扩容
        耗时过多的情况，我们可以将扩容穿插在插入操作的过程中，分批完成。
        当装载因子触达阈值之后，我们只申请新的空间，但并不是将老的数据
        搬迁到新三联表中。
        
        当有新数据插入时，我们将数据插入新散列表中，并且从老的散列表中
        拿出一个数据放入新散列表。每次插入一个数据到散列表，我们都重复
        这个过程。经过多次插入操作之后，老的散列表中的数据就一点一点
        全部搬迁到新散列表中了。这样就没有了集中的一次性数据搬迁，插入
        操作就会变得很快。
        
        这个期间，对于查询操作，为了兼容新、老散列表中的数据，我们先从新
        的散列表中查找，如果没有找到，再去老的散列表中查找。
        
    5、如何选择冲突解决办法？
    
        基于开放寻址法：
            
            数据比较小，装载因子比较小的时候，合适采用开发寻址法。
            这也是 java 中 ThreadLocalMap 使用开发寻址法解决
            散列冲突的原因。
        
        链表法：
        
            基于链表的散列冲突处理方法比较合适存储大对象，大数据量的散列表，
            而且，比起开发寻址法，它更加灵活，支持更多的优化策略，比如用
            红黑树代替链表。
            
    6、小结：
    
        工业级的散列表应具备哪些特点：
        
            a. 支持快速查询、插入、删除操作。
            
            b. 内存占用合理、不能浪费过多内存空间。
            
            c. 性能稳定、极端情况下，散列表的性能也不会退化到无法接受的情况。
            
        如何实现这样的散列表：
        
            a. 设计一个合理的散列函数
            
            b. 定于装载因子阀值，并且设计动态扩容策略
            
            c. 选择合适的散列冲突解决方法。     
        
        
        
"""20| 散列表(下): 为什么散列表和链表经常会一起使用"""            
    
    在链表那一节中，我们提到，借助散列表，可以把 LRU 缓存淘汰算法时间
    复杂度减低为 O(1)。现在我们看是如何做到的。    
    
    1、链表实现的 LRU(Least recently used) 缓存淘汰算法:
    
        在链表那一节中，我们提到，借助散列表，可以把 LRU 缓存淘汰算法时间
        复杂度减低为 O(1)。现在我们看是如何做到的。        
     
        我们需要维护一个按照访问时间从大到小有序排列的链表结构。
        因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，
        我们就直接将链表头部的结点删除。
        
        当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，
        则直接将数据放到链表尾部；如果找到了，我们将它移动到链表尾部。
        因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰
        算法的时间复杂度很高，是 O(n)。
        
        实际上，一个缓存系统主要包含下面几个操作：
            从缓存中添加一个数据
            从缓存中删除一个数据
            从缓存中查找一个数据
        这三个操作都设计到"查找"操作，如果单纯用链表的话，时间复杂度就是O(n)。
        如果我们将散列表和链表两种数据结构组合使用，可以将三种操作的时间复杂度
        都降到 O(1)。
        
    2、链表和散列表实现 LRU 缓存淘汰算法：
    
        我们使用双向链表存储数据，链表中的每个结点处理存储数据(data)、
        前驱指针(prev)、后继指针(next)之外，还增加了一个特殊的字段 hnext。
        这个 hnext 有什么用呢？
        
        因为我们使用散列表时通过链表法解决散列冲突的，所以每个结点都会在两条链中。
        一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。
        前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。
        
        首先，我们看如何查找一个数据：
            
            通过散列表，我们很快地在缓存中找到一个数据，
            当找到数据之后，我们还需要将它移动到双向链表的尾部。
        
        其次，删除一个数据：
        
            借助散列表，找到要删除的数据

        最后，如何添加一个数据：
        
            先看数据是否存在，如果存储将其移动到双向链表的尾部。
            如果不存在，缓存没有满，直接将数据放到链表尾部
            如果不存在，缓存满了，删除双向链表头的结点，然后再将数据放到链表尾部。
            
                          

"""21| 哈希算法(上): 如何防止数据库中的用户信息脱库 """     
    
    1、什么是哈希算法：
        
        实际上，不管是"散列"还是"哈希"都是中文翻译的差别，
        英文其实就是"Hash"。所以，我们常听到有人把“散列表”
        叫作“哈希表”“Hash 表”，把“哈希算法”叫作“Hash 算法”
        或者“散列算法”。那么到底什么是哈希算法呢？
        
        将任意长度的二进制值串映射为固定长度的二进制值串，
        这个映射的规则就是哈希算法，而通过原始数据映射之后
        得到的二进制值串就是哈希值。
        
        设计一个优秀的哈希算法需要满足一下几点：
            
            a. 从哈希值不能反推导出原始数据(哈希算法也叫单向哈希算法)
            
            b. 对输入数据非常敏感，哪怕原始数据只修改一个Bit，最后得到的
                哈希值也大不相同。
                
            c. 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小。
            
            d. 哈希算法的执行效率尽量高效，针对较长的文本，也能快速计算出哈希值。
            
    2、 哈希算法的应用非常多，最常见的有
        安全加密 ：
            最常用用于加密的哈希算法是：
            MD5(MD5 Message-Digest-Algorithm, MD5 消息摘要算法)
            SHA(Secure Hase Algorithm, 安全散列算法)
        
        唯一标识：
            数据的唯一性
        
        数据校验：
            数据的敏感度，原始数据修改一点，哈希值就不同。
        
        散列函数：
            散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表
            的性能。不过，相对哈希算法的其他应用，散列函数对散列冲突的要求要低很多，
            即便出现散列冲突，只要不严重，就可通过开放寻址法和链表法解决。
            不仅如此，散列函数对于散列算法计算得到的值，是否反向解密也并不关心。
            散列函数得到散列值更关心散列后的值是否平均分布。
        
        负载均衡：
        
        数据分片：
        
        分布式存储：
    
    3、小结：
    
        唯一标识：
            哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。
            
    
"""22| 哈希算法(下): 哈希算法在分布式系统中有哪些应用"""  

    哈希算法如何解决分布式问题：
    
    1、负载均衡：
        
        我们知道，负载均衡的算法有很多，比如轮询、随机、加权轮询等，
        那如何才能实现一种会话沾滞(session sticky)的负载均衡算法呢？
        也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都
        路由到同一个服务器上。
        
        最简单的方法是维护一张关系表，这张表的内容是客户端IP/会话ID
        与服务器变化的映射关系。这种方式简单直观，但是弊端是：
            
            如果客户端很多，映射表会很大，比较浪费内存
            
            客户端下线，上线，服务器扩容，缩容都会导致映射失效，
            这样维护表的成本很高。
        
        我们通过哈希算法，对客户端IP 地址或者会话 ID 计算哈希值，
        将取得的哈希值与服务器列表的大小进行取模运算，最终得到的
        值就是应该被路由到的服务器编号。这样就可以把同一个IP过来
        的所有请求，都路由到同一个后端服务器上。
        
    
    2、数据分片：
    
        例如：如何统计“搜索关键词”出现的次数:
        
            先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。
            处理过程也就是 MapReduce 的比本设计思想。
        
    3、分布式存储：
    
        为了提高数据的读写能力，一般采用分布式的方式来存储数据。
        
        一致性哈希算法：
            假设我们有 k 个机器，数据的哈希值范围是 [0, MAX]
            我们将整个范围划分成 m 个小区间(m, 远大于k),
            每个机器负责 m/k 个小区间。
            当有新机器加入的时候，我们就将某几个小区间的数据，从
            原来的机器中搬迁至新的机器中，这样，既不用全部重新哈希、
            搬迁数据、也保持了各个机器上数据数量的均衡。
            
"""23| 二叉树基础(上): 什么样的二叉树适合用数组来存储？"""

    思考题：二叉树有哪几种存储方式，什么样的二叉树适合用数组来存储？
    
    1、树(Tree)：
        
        关于树，有三个比较相似的概念：高度(Height)、深度(Depth)、层(Level)：
        
        节点的高度 = 节点到叶子节点的最长路径(边数)
        节点的深度 = 根节点到这个节点所经历的边的个数
        节点的层数 = 节点的深度 +  1
        树的高度 = 根节点的高度
        
        “高度”这个概念，其实就是从下往上的度量，起点是叶子节点(0开始)。
        “深度”这个概念，是从上往下的度量，从根节点开始度量，计数起点是0。
        层与深度类似，不过，计数起点是 1。
        
    2、二叉树(Binary Tree):
    
        二叉树，顾名思义，每个节点最多有两个"叉"，也就是两个节点，
        分别是左子节点和右子节点。
        
        满二叉树：叶子节点全都在最底层，除了叶子节点之外，每个节点都有
                左右两个子节点。
                
        完全二叉树：叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，
                    并除了最后一层，其他层的节点个数都要达到最大。
                    
        要完全理解二叉树的由来，需要先了解，如何表示(或者存储)一颗二叉树。
        想要存储一颗二叉树，一种是基于指针或者引用的二叉链式存储法，
        一种是基于数组的顺序存储法。
        
        链式存储法：
            每个节点都有三个字段，其中一个存储数据，另外两个是指向左右子节点
            的指针。我们只要拎住根节点，就可以通过左右节点的指针，把整个树都串
            起来。
            
        顺序存储法：
            
            我们把根节点存在下标 i = 1的位置，那左子节点存储在下标 2*i = 2 的位置。
            右子节点存储在 2*i + 1 = 3 的位置。
            以此类推，第二层左子节点的左子节点存储在 2*i = 2*2 = 4 的位置，右子节点
            存在 2*i + 1 = 2*2 + 1 = 5 的位置。
            
            如果节点 X 的存储在数组中的下标为 i 的位置，下标为 2*i 的位置存储的就是
            左子节点，下标 2*i + 1 的位置存储的就是右子节点。
            为了计算方便，根节点会存储在下面为 1 的位置。
            
            一颗完全二叉树仅仅浪费了一个下标为 0 的存储位置。
            如果是非完全二叉树，其实会浪费比较多的数组存储空间。
            
            所以，如果某棵二叉树是一棵完全二叉树，那么用数组存储无疑是最节省内存的一种
            方式，这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求
            最后一层的子节点都靠左的原因。
            
    3、二叉树的遍历：
    
        前序遍历、中序遍历和后序遍历。其中，前、中、后，表示节点与它的左右子树
        节点遍历打印的先后顺序。
        
        a. 前序遍历是指：
            对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印右子树。
        
        b. 中序遍历是指：
            对于树中的任意节点来说，先打印它在左子树，然后再打印它本身，做好打印它的右子树。
            
        c. 后序遍历是指：
            对于树中的任意节点来说，先打印它的左子树，再打印它的右子树，最后打印这个节点。
            
        
        实际上，二叉树的前、中、后序遍历就是一个递归过程。
        写递归代码的关键，就是看能不能写成递推公式，
        而写递推公式的关键就是，如果要解决问题 A,就假设子问题 B、C已经解决，
        然后再看如何利用 B、C 来解决A。
        
        前序遍历：
            void preOrder(Node *root):
                if(root == null) return;
                
                print root; //伪码，表示打印 root 节点。
                preOrder(root->left);
                preOrder(root->right);
            
        中序遍历：
            void preOrder(Node *root):
                if(root == null) return;
               
                preOrder(root->left);
                print root; //伪码，表示打印 root 节点。
                preOrder(root->right);    
        
        后序遍历：
            void preOrder(Node *root):
                if(root == null) return;
                
                preOrder(root->left);
                preOrder(root->right);                 
                print root; //伪码，表示打印 root 节点。
                    
                    
        遍历的时间复杂度：
        
            每个节点最多被访问两次，所以遍历操作是时间复杂度和节点
            的个数 n 成正比，也就是说二叉树遍历的时间复杂度为 O(n)。
       
            
            
"""24| 二叉树基础(下)：有了如此高效的散列表，为什么还需要二叉树"""

    1、二叉查找树：
    
        二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。
        二叉查找树就是为了实现快速查找而生的。
        不仅仅支持快速查找，还支持快速插入、删除一个数据。
        
        这些实现都依赖二叉查找树的特殊结构。
        二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，
        都要小于这个节点的值，而右子树节点的值都要大于这个节点的值。
        
    2、二叉查找树的查找操作：
        
        首先，我们先取一个节点，如果它等于我们要查找的数据，那就返回。
        如果要查找的数据比根节点的值小，那就在左子树中查找；
        如果要查找是数据比根节点的值大，那就在右子树中递归查找。
        
        
        def find(self, data):
            treeNode = self.root
            print(treeNode.elem)
    
            while treeNode:
    
                print(treeNode.elem)
                if data == treeNode.elem:
                    return treeNode
                elif data < treeNode.elem:
                    treeNode = treeNode.lchild
                elif data > treeNode.elem:
                    treeNode = treeNode.rchild
    
            return None  
        
    3、二叉树的插入操作：
    
        二叉查找树的插入过程有点类似查找操作，新插入的数据一般都在叶子节点上，
        所以，我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。
        
        如果要插入的数据比节点的数据大，并且节点的右子树为空，就将数据之间插入
        到右子节点的位置。如果不为空，就再递归遍历右子树，查找插入位置。
            
        def insert(self, data):
            node = Node(data)
            treeNode = self.root
            while treeNode:
                
                if data > treeNode.elem:
                    if treeNode.rchild is None:
                        treeNode.rchild = node
                        return
                    treeNode = treeNode.rchild
                
                else:
                    if treeNode.lchild is None:
                        treeNode.lchild = node
                        return 
                    treeNode = treeNode.lchild    
              
              
    4、二叉查找树的删除操作：                
       
       二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂。
       针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。
       
       第一种情况，如果要删除的节点没有子节点，我们只需要直接将父节点中，
       指向要删除节点的指针置为 null。
       
       第二种情况，如果要删除的节点只有一个子节点，我们只需要更新父节点中，
       指向要删除节点的指针，让它指向要删除节点的子节点就可以了。
       
       第三种情况，如果要删除的节点有两个子节点，这就比较复杂。
       我们需要找到这个节点的右子树中的最小节点，把它替换到
       要删除的节点上。然后删除点这个最小节点，因为最小节点肯定
       没有左子节点。
       
        def delete(self, data):
        
            # treeNode 指向要删除的节点，初始化指向根节点
            treeNode = self.root 
            ftreeNode = None   # ftreeNode 记录treeNode 的父节点。
            
            # 寻找要删除的节点。
            while treeNode and treeNode.elem != data:
                ftreeNode = treeNode
                if data > treeNode.elem:
                    treeNode = treeNode.rchild
                else:
                    treeNode = treeNode.lchild
            
            if treeNode is None:
                return   # 没有找到
            
            # 要删除的节点有两个子节点
            if treeNode.lchild  and  treeNode.rchild:
                minP = treeNode.rchild
                minPP = treeNode  # minPP 表示 minP 的父节点
                while minP.lchild:
                    minPP = minP
                    minP = minP.lchild
                    
                treeNode.elem = minP.elem  # 将 minP 的数据替换到treeNode 中。
                treeNode = minP  # 下面就变成了删除 minP 了。
                ftreeNode = minPP
                
            # 删除节点是叶子节点或者仅有一个节点
            
            child = None
            if treeNode.lchild:
                child = treeNode.lchild
            elif treeNode.rchild:
                child = treeNode.rchild
            else:
                child = None
            
            if ftreeNode is None:
                self.root = child  # 删除根节点
            elif ftreeNode.lchild == treeNode:
                ftreeNode.lchild = child
            elif
                ftreeNode.rchild = child
            
            
        实际上，关于二叉搜索树的删除操作，还有非常简单，取巧的方法，
        就是单纯将要删除的节点标记为“已删除 ，但并不是从书中将这个
        节点去掉。
        这样原本删除的节点还需要存储在内存中，比较浪费内存空间。
        但是删除操作就变得简单很多。而且这种处理方法也没有增加插入
        查找操作代理实现的难度。
        
    5、二叉查找树的其他操作：
    
        二叉查找树还支持快速查找最大节点、最小节点、前驱节点、后继节点。
        
        二叉查找树除了上面几个操作之外，还有一个重要的特性，就是中序遍历
        二叉查找树，可以输入有序的数据序列，时间复杂是O(n),非常高效。
        因此，二叉查找树也叫二叉排序树。           


    6、支持重复数据的二叉查找树：
    
        前面讲到二叉查找树的时候，我们默认树中节点存储都是数字，很多时候，
        在实际软件开发中，我们在二叉树中存储的，是一个包含很多字段的对象。
        我们利用对象的某个字段作为键值(Key)来构建二叉树，我们把对象中的
        其他字段叫作卫星数据。
        
        如果存储的两个对象键值相同，这种情况该怎么处理？
        
        第一种方法比较容易：
            二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和
            支持动态扩容的数据等数据结构，把值相同的数据都存储在同一个节点上。
            
        第二种方法比较不好理解：
            
            每个节点仍然只存储一个数据，在查找插入位置的过程中，如果碰到一个节点
            的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的
            右子树，也就是说，把这个新插入的数据当做大于这个节点的值来处理。
            
            当查找数据时，遇到相同的节点，我们不停止查找操作，而是继续在右子树
            中查找，直到遇到叶子节点，才停止。
            
            对于删除操作，我们也需要先查找到每个要删除的节点，然后在按前面讲
            的删除操作方法，依次删除。
            
            
    7、二叉查找树的时间复杂度分析：
    
        查找时间复杂度：
            
            对于根节点的左右子树极度不平衡，意境退化成了链表，所以查找的世界
            复杂度是 O(n)。
            
            二叉查找树的插入、删除、查找的时间复杂度其实都根树的高度成正比，
            也就是 O(height)。
            
            树的高度等于最大层减一，为了方面计算，我们转换成层来表示。
            
            包含 n 个节点的完全二叉树，第一次包含 1 个节点，第二层包含 2 个节点，
            第 k 层包含的节点个数就是 2^(k - 1)。
            
            但是对于完全二叉树来说，最后一层的节点个数有点不遵守上面的规律。
            它包含的节点个数在 1 个到 2^(L - 1) 个之间。假设L为最大层。
            
            如果我们把每一层的节点个数加起来就是总节点个数 n。
            也就是说，如果节点的个数是 n, 那么 n 满足下面这个关系：
            
            n >= 1 + 2 + 4 + ... + 2^(L - 2) + 1
            n <= 1 + 2 + 4 + ... + 2^(L - 2) + 2^(L - 1)
            
            借助等比数列求和公式：
            L 的范围是 [log2(n+1), log2n + 1]。
            完全二叉树的层数小于等于 log2n + 1
            完全二叉树的高度小于等于 log2n
            
        平衡二叉树的高度接近 logn,所以，插入、删除、查找操作的时间复杂度是O(logn)。
        
    8、有了散列表，我们为什么还需要二叉树查找树：
    
        第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。
        对于二叉查找树，我们只需要中序遍历，就可以在 O(n)的时间复杂度内，输出有序数据序列。
        
        第二，散列表扩容耗时很多，而且到遇到散列冲突时，性能不稳定。尽管二叉树的性能不稳定
        但在工程中，我们使用平衡二叉查找树的性能非常稳定。时间复杂稳定的 O(logn)。
        
        第三，尽管散列表的查找等操作的世界复杂度是常量级的，因为哈希冲突的存在，这个常量
        不一定比logn 小，所以实际的查找速度可能不一定比 O(logn)快，加上哈希函数的耗时，
        也不一定就比平衡二叉查找树的效率高。
        
        第四，散列表的构造比二叉操作树要复杂，需要考虑很多东西。
        比如散列函数的设计、冲突解决办法、扩容、缩容。
        平衡二叉查找树只需要考虑平衡性这个问题，而且这个问题的解决办法比较成熟、固定。
        
        最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法
        解决冲突的散列表，不然会浪费一定的存储空间。
        
        综合几点，平衡二叉查找树在某些方面还是优于散列表的，所以，二者并存并不冲突。
        时间开发中，需要结合实际选择需要哪个。
    
    
    
"""25| 红黑树(上):为什么工程中都用红黑树这种二叉树 """                

    二叉树在频繁的动态更新过程中，可能会出现数的高度远大于 log2N的情况
    从而导致各个操作的效率下降。要解决这个复杂度退化的问题，我们设计一种
    平衡二叉查找树。
    
    但凡将到平衡二叉查找树，就会拿红黑树做为例子，为什么工程中都喜欢用红黑树
    而不是其他平衡二叉树。
    
    1、什么是“平衡二叉查找树”？
    
        平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于 1。
        全二叉树，满二叉树其实都是平衡二叉树，但是非完全二叉树
        也可能是平衡二叉树。
        
        平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点。
        最先被发明的平衡二叉查找树是AVL树。
        
        但是很多平衡二叉查找树其实没有严格符合上面的定义(树中任意一个节点的左右子树的
        高度相差不能大于1)，比如，我们下面将的红黑树，它从根节点到各个叶子节点的最长
        路径，可能会比最短路径大一倍。
        
        学习数据结构和算法为了实际应用到开发中，没有必要死抠定义，对于平衡二叉查找树这个
        概念，我们要从这个数据结构的由来，去理解“平衡”的意思。
        
        发明平衡二叉树的这类数据结构的初衷是解决普通二叉查找树在频繁插入、删除等动态更新
        的情况下，出现时间复杂度退化的问题。
        所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，
        不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些。
        相应的插入、删除、查找等操作的效率高一些。
        
    2、如何定义一颗“红黑树”？
    
        红黑树“Red-Black Tree”，简称 R-B Tree, 它是一种不严格的平衡二叉查找树。
        红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一颗红黑树
        还需要满足这样几个要求：
            a. 根节点是黑色的。
            b. 每个叶子节点都是黑色的空节点(NIL)，也就是说，叶子节点不存储数据。
                主要是简化红黑树的代码实现而设置的。
            c. 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的。
            d. 每个节点，从该节点到达其可达叶子节点的多有路径，都包含相同数目的黑色节点。
            
    3、为什么说红黑树是“近似平衡的”
        
        “平衡”的意思可以等价为性能不退化。
        “近似平衡”就等价为性能不好退化的太严重。
        
        我们看看红黑树的时间复杂度，也就是说看看高度。
        首先，我们将红色节点从红黑树中去掉，那么黑色节点的红黑树的高度树多少呢？
        红黑树中：从任意节点到可达叶子节点的每个路径包含相同数据的黑色节点。
        那么只包含黑色节点是黑树比包含相同节点个数的完全二叉树的高度还要小。
        完全二叉树的高度近似为 log2N,所以去掉红色节点的黑树高度不会超过 log2N。
        加入红色节点之后，最长路径不会超过 2log2N,也就是说，红黑树的高度近似为2log2N。
        
    4、开篇解答：
    
        AVL 树是一种高度平衡的二叉树，查找效率很高，但是有利就有弊，AVL 树为了维持
        这种高度的平衡，付出了很多代价，每次插入、删除都有调整，比较复杂。
        
        红黑树只是做了近似平衡，并不严格的平衡，维护成本比AVL树要低。
        但是，插入、删除、查找操作性能都比较稳定。
        
        我们学习数据结构要学习它的由来、特性、使用的场景以及解决的问题。
        红黑树是一种平衡二叉查找树，是为了解决普通二叉查找树在数据更新过程中
        复杂度退化的问题而产生的。红黑树的高度近似 log2N，所以，它是近似平衡，
        插入、删除、查找操作时间复杂度都是O(logN).
         
    5、动态数据结构支持动态地数据插入、删除、查找操作，除了红黑树，还有那些：
    
        动态数据结构是支持动态的更新操作，里面存储的数据是时刻在变化的，
        通俗的将，它不仅仅支持查询，还支持删除、插入数据。
        而且，这些操作都非常高效。
        如果不高效，就算不上有效的动态数据结构了。
        链表、栈、队列实际上都不算动态数据结构。    
        散列表、跳表、红黑树算是动态数据结构。
 
                    
"""26|红黑树(下): 掌握这些技巧，你也可以实现一个红黑树 """

    1、红黑树的基本思想：
        红黑树的平衡过程大致就是：遇到什么样的节点排布，我们就对应怎么去调整。
        只要按照这些固定的调整操作，就能将一个非平衡的红黑树调整成平衡的。
        
    
       
"""27|递归树: 如何借助树来求解递归算法的时间复杂度"""

    1、递归树与时间复杂度分析：
    
        递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小问题来求解。
        这样一层一层地分解，直到问题的数据规模被分解到足够小，不用继续递归分解为止。
        
        如果我们把这一层一层的分解过程画成图，它其实就是一颗树。
        我们给这颗树起一个名字，叫做递归树。
        
        
        
"""28| 堆和堆排序：为什么数堆排序没有快速排序快"""                    
   
    堆(heap)：一种特殊的树, 堆数据结构应用场景很多，最经典的莫过于堆排序。
    堆排序是一种原地、时间复杂度为 O(nlogn)的排序算法。
    
    快排也是 O(nlogn)，为什么在实际应用中快排排序的性能要比堆排序好。
    
    1、如何理解“堆”：
    
        堆是一种特殊的树，什么样的树才是堆：
            a. 堆是一个完全二叉树。（除最后一层，其他层节点个数是满的，最后一层的节点都靠左排列）
            b. 堆中的每个节点的值都必须大于等于(小于等于)其子树中每个节点的值。
                (我们可以换个说法，堆中每个节点的值都大于等于(或者小于等于)其左右子节点的值。
                大于等于的是大顶堆，小于等于的是小顶堆)
        
    2、如何实现一个“堆”：
    
        要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆。
        完全二叉树比较适合用数组来存储，非常节省空间。
        根节点放下标为 1 的位置，第i个节点的左子节点为 2i, 右子节点为 2i+1。
        父节点的下标为 i/2。
        堆用数组实现。
       
        a. 往堆中插入一个元素（大顶堆为例）：
            
            如果往把新插入的元素放到堆后面，是不符合堆特性的，我们需要对其调整
            让其重新满足对的特性，这个过程我们称为堆化(heapify)。
            
            堆化实际上有两种，从下往上和从上往下。
            从下往上的堆化方法：
            堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。
            我们可以让新插入的节点与父节点对比大小，如果不满足子节点小于等于父节点
            的大小关系，我们就互换两个节点，一直重复这个过程，知道父子节点之间
            满足刚说的那种大小关系。
            
            public class Heap{
                private int[] a; //数组,从下标 1 开始存储数据
                private int n;   //堆中可以存放数据的个数
                private int count; // 堆中已经存储的数据个数
                
                public Heap(int capacity){
                    a = new int[capacity + 1];
                    n = capacity;    
                    count = 0;
                }
                
                public void insert(int data){
                    if(count >= n)return;
                    ++count;
                    a[count] = data;
                    int i = count;
                
                    while(i/2 > 0 && a[i] > a[i/2]){ //自下向上堆化
                        swap(a, i, i/2);  // swap()函数作用，交换下标 i 和 i/2的两个元素。
                        i = i/2;
                    }           
                        
                }
            }
            
        b. 删除堆顶元素：
            
            从堆的定义第二条，可以知道，堆顶元素就是堆中数据的最大值或者最小值。
            
            假设我们构建大顶堆，堆顶元素就是最大的元素，当我们删除堆顶元素之后，
            就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。
            然后我们删除第二大元素，以及类推，直到叶子节点被删除。
            
            删除堆顶元素，为了满足慢二叉树的特性，我们把最后一个元素放入堆顶。
            然后利用同样父子节点大小关系，互换两个节点，并且重复进行这样的过程
            直到父子节点之间满足大小关系为止，这个就是从上往下堆化。
            
            因为我们移除的是最后一个元素，而且在堆化的过程中都是交换，
            不会出现数组的空洞，所以堆化出来的结果，肯定满足完全二叉树特性。
            
            public void removeMax(){
                
                if(count == 0) return -1; //堆中没有数据
                a[1] = a[count];  //最后一位放入堆顶
                --count;
                heapify(a, count, 1);
            
            }
            
            private void heapify(int [], int n, int i){ // 自上往下堆化
                while(true){
                    int maxPos = i;
                    if(i*2 <=n && a[i] < a[i*2]) maxPos = i*2;
                    else if(i*2+1 <=n && a[maxPos] < a[i*2+1]) maxPos = i*2+1;
                    else if(maxPos == i) break;
                    swap(a, i, maxPos);
                    i = maxPos;
                
                }
            
            
            
            }
            
 
            
        c. 插入、删除的时间复杂度：
        
            一个包含 n 个节点的完全二叉树，树的高度不会超过 log2n,
            堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间
            复杂度根树的高度成正比，也就是O(logn)。
            堆中插入和删除的时间复杂度都是 O(logn)。
            
    3、如何基于堆实现排序？
    
        借助堆这种数据结构实现的排序算法就是堆排序。
        堆排序过程大致分解成两个大的步骤：
        a. 建堆：
        
            首先，我们将数组原地建成一个堆，所谓“原地”，就是不借助另
            一个数组，就在原数组上操作。
            
            建堆的两种思路，第一种借助我们前面讲的，在堆中插入一个元素的思路。
            尽管数据中包含 n 个数据，但我们可以假设，起初堆中只包含一个数据，
            就是下标为 1 的数据。然后调用插入操作，将下标 2 到 n 的数据依次插入
            到堆中，这样我们将将包含 n 个数据的数组，组织成了堆。
            
            另一个思路，从后往前处理数据，并且每个数据都是从上往下堆化。
            因为子叶子往下堆化只能自己跟自己比较，所以，我们直接从第一个
            非叶子节点开始，依次堆化就行了。
            
            private static void buildHeap(int[] a, int n){
                for(int i = n/2; i>= 1; --i){
                    heapify(a, n, i);
                }
            }
            
            private static void heapify(int[] a, int n, int i){
                while(true){
                    int maxPos = i;
                    if(i*2 <= n && a[i]< a[i*2]) maxPos = i*2;
                    else if(i*2 + 1 <= n && a[i]<a[i*2+1]) maxPos = i*2 + 1;
                    else if(maxPos == i) beak;
                    i = maxPos;
                }
            }
            
            在这段代码中，我们对下标 n/2 开始到 1 的数据进行堆化，下标 n/2+1 到 n 的
            节点是叶子节点，我们不需要堆化。实际上，对于一个二叉树，下标从 n/2+1 到 n
            的节点都是叶子节点。
            
          
            建堆的时间复杂度是多少？
            因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始，每个节点堆化过程
            需要比较和交换节点的个数，跟这个节点的高度 k 成正比。
            
            建堆的时间复杂度是 O(n)。

        b. 排序：
        
            建堆结束之后，数组中的数据已经是按照大顶堆来构建，我们把数组中的第一个元素
            跟最后一个元素交换，那最大的元素就放到下标为 n 的位置。
            把下标为 n 的元素放到了堆顶，然后再通过堆化的方法，将剩下的 n - 1 个元素重新
            构建成堆。堆化完成后，再取堆顶元素放到 n - 1 的位置，一直重复这个过程，
            直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。
            
            // n 表示数据的个数，数组 a 中的数据从下标 1 到 n 的位置。
            public static void sort(int[] a, int n){
                buildHeap(a, n);
                int k = n;
                while(k > 1){
                    swap(a, 1, k);
                    --k;
                    heapify(a, k, 1);
                }
            
            }
                            
        c. 时间复杂度：
        
            建堆过程时间复杂度是 O(n), 排序过程的时间复杂度是 O(nlogn),
            所以，堆排序的时间复杂度是 O(nlogn)。
            
    4、小结：
    
        快速排序为啥比快排好？
        
            第一点，堆排序数据访问的方式没有快排友好。
                对于快排，数据是顺序访问的，堆排序是跳着访问的。
                对CPU 缓存不友好。
                
            第二点，同样的数据，堆排序算法的数据交换次数要多于快排序。        



"""29| 堆的应用: 如何快速获取到Top 10 最热门的搜索关键字"""        

    1、堆的第一个应用: 优先级队列
    
        队列的最大特性是先进先出，优先队列，数据的出队顺序不是先进先出，
        而是按照优先级来，优先级最高的，最先出队。
        
        如何实现一个优先队列呢？方法有很多，但是用堆实现是最直接，最高效的。
        因为，堆和优先队列非常相似，一个堆就可以看作一个优先队列。很多时候，
        他们只是概念上的区分而已，往优先队列中插入一个元素，就相当于往堆中
        插入一个元素，从优先队列中取出优先级最高的元素，就相当于取出堆顶元素。
        
        优先队列的实现 Java 中的PriorityQueue, C++ 中的 priority_queue。
        
    2、堆的第二个应用: 利用堆求 Top K
    
    3、堆的第三个应用：利用堆求中位数：
    
        中位数，顾名思义，就是处于中间位置的那个数。
        如果数据的个数是奇数，把数据从小到大排序，那第 n/2 + 1 个数据就是中位数。
        如果数据的个数是偶数的话，那处于中间位置的数据有两个，第 n/2 和 n/2+1个数据。
        
        如果面对的是动态数据集合，中位数在不停的变动，如果再用先排序，再询问中位数
        那效率就很低。
        
        借助堆这种数据结构，我们不用排序，就可以非常高效实现中位数操作。
        我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆存储前半部分数据，
        小顶堆存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。
        
        如果有n个数据，n是偶数，前 n/2 存储在大顶堆，后 n/2 存储在小顶堆中。
        如果n是奇数，大顶堆中存储 n/2 + 1个，小顶堆中存储 n/2 个数据。
        
        如果新加入的数据小于等于大顶堆的对顶元素，我们就插入到大顶堆中，
        如果是大于等于小顶堆的元素，就我们就插入到小顶堆中。
        这个时候可能会出现两个堆中的数据不符合前面约定的情况：
        如果有n个数据，n是偶数，前 n/2 存储在大顶堆，后 n/2 存储在小顶堆中。
        如果n是奇数，大顶堆中存储 n/2 + 1个，小顶堆中存储 n/2 个数据。
        这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆中，
        通过这样的调整，来让两个堆中的数据满足上面的约定。
        
        插入数据设计到堆化时间复杂度是O(logn),但是中位数值需要
        返回大顶堆的堆顶元素就可以了，所以时间复杂度是O(1)。
        
        
        
"""30| 图的表示：如何存储微博、微信等社交网络中的好友关系"""        
        
        
        
        
        
        
                    
            
            
            
            
                
            
             
            
            
                    
                 
            
            
   
   
                            
            
            
        
                
                         
                   
        
    
            
        
              
                    
                
                    
                
            
                
                     
                            
        
         
            
                   
       
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        